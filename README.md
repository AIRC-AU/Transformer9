# English-Hungarian Neural Machine Translation with Knowledge Distillation

A complete implementation of Transformer-based neural machine translation from English to Hungarian, enhanced with an improved knowledge distillation system for model compression and acceleration.

## ğŸ¯ Project Overview

This project implements a state-of-the-art neural machine translation system that:
- Translates English text to Hungarian using Transformer architecture
- Uses an improved knowledge distillation system to create efficient compressed models
- Provides multiple training configurations for different needs
- Includes complete training, testing, and evaluation pipeline

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cmn_hun.py                      # Data processing for English-Hungarian translation
â”‚   â””â”€â”€ hun-eng/                        # Hungarian-English parallel corpus
â”œâ”€â”€ model/
â”‚   â””â”€â”€ transformer.py                  # Transformer model implementation
â”œâ”€â”€ train_cmn_hun_transformer.py        # Teacher model training script
â”œâ”€â”€ improved_distillation_trainer.py    # Improved knowledge distillation training
â”œâ”€â”€ test_trained_student.py             # Student model testing and evaluation
â”œâ”€â”€ inference_cmn_hun_transformer.py    # Translation inference script
â””â”€â”€ train_process/                      # Training outputs and checkpoints
    â”œâ”€â”€ transformer-cmn-hun/            # Teacher model checkpoints
    â””â”€â”€ distillation-[config]/          # Student model checkpoints by configuration
```

## ğŸš€ Quick Start

### 1. Train Teacher Model
```bash
python train_cmn_hun_transformer.py
```

### 2. Knowledge Distillation Training
```bash
# Run the improved distillation trainer
python improved_distillation_trainer.py

# Choose from 5 configurations:
# 1. Quick Test (15 minutes) - for verification
# 2. Conservative (2 hours) - stable training
# 3. Balanced (3 hours) - recommended
# 4. Aggressive (5 hours) - high compression
# 5. High Quality (8 hours) - best results
```

### 3. Test Student Model
```bash
python test_trained_student.py --model_path ./train_process/distillation-[config]/checkpoints/student_best.pt
```

### 4. Translation Inference
```bash
python inference_cmn_hun_transformer.py
```

## ğŸ“Š Training Configurations

### Available Configurations
| Configuration | Training Time | Compression Ratio | Use Case |
|---------------|---------------|-------------------|----------|
| **Quick Test** | 15 minutes | 1.8x | Code verification |
| **Conservative** | 2 hours | 1.7x | Stable training |
| **Balanced** | 3 hours | 2.0x | Recommended |
| **Aggressive** | 5 hours | 3.0x | High compression |
| **High Quality** | 8 hours | 1.8x | Best results |

### Expected Performance
- **Model Compression**: 1.7x - 3.0x depending on configuration
- **Inference Speedup**: 1.5x - 3.0x faster than teacher model
- **Training Time**: 15 minutes - 8 hours based on quality requirements

## ğŸ› ï¸ Technical Features

### Improved Knowledge Distillation
- **Multiple Configurations**: 5 different training modes
- **Advanced Loss Function**: Label smoothing + improved masking
- **Optimized Training**: Cosine annealing + gradient accumulation
- **Comprehensive Monitoring**: TensorBoard integration + detailed logging

### Model Architecture
- **Transformer Encoder-Decoder**: Standard architecture with optimizations
- **Dynamic Compression**: Configurable compression ratios (1.7x - 3.0x)
- **Flexible Sequence Length**: Adaptive to different configurations
- **Vocabulary**: English and Hungarian tokenizers with caching

### Training Optimizations
- **AdamW Optimizer**: With weight decay and advanced scheduling
- **Gradient Management**: Clipping + accumulation for stability
- **Error Handling**: Comprehensive exception handling and recovery
- **Automatic Saving**: Best model selection and checkpoint management

## ğŸ¯ Key Files Description

### Core Training Files
- **`train_cmn_hun_transformer.py`**: Train the teacher model from scratch
- **`improved_distillation_trainer.py`**: Advanced knowledge distillation with 5 configurations
- **`test_trained_student.py`**: Comprehensive testing and evaluation of student models

### Model & Data Files
- **`model/transformer.py`**: Transformer model implementation with optimizations
- **`data/cmn_hun.py`**: English-Hungarian data processing and loading
- **`inference_cmn_hun_transformer.py`**: Translation interface for both models

## ğŸ“ˆ Usage Examples

### Training Workflow
```bash
# 1. Train teacher model (if not already trained)
python train_cmn_hun_transformer.py

# 2. Run knowledge distillation
python improved_distillation_trainer.py
# Choose configuration: 3 (Balanced - recommended)

# 3. Test the trained student model
python test_trained_student.py --model_path ./train_process/distillation-balanced/checkpoints/student_best.pt

# 4. Use for translation
python inference_cmn_hun_transformer.py
```

### Interactive Translation
```bash
# Start interactive translator
python inference_cmn_hun_transformer.py

# Input examples:
en:Hello world
hun:JÃ³ reggelt!
```

## ğŸ”§ Requirements

- Python 3.8+
- PyTorch 1.12+
- torchtext
- tqdm
- tensorboard
- numpy

## ğŸ‰ Project Highlights

This project successfully demonstrates:
- **Advanced Knowledge Distillation**: Multiple training configurations for different needs
- **Flexible Compression**: 1.7x - 3.0x model compression options
- **Improved Training**: Stable and reliable distillation process
- **Complete Pipeline**: From teacher training to student evaluation
- **Production Ready**: Professional-grade training and testing tools

## ğŸ“– Documentation

- `TRAINING_CONFIGURATIONS_GUIDE.md`: Detailed configuration guide
- `IMPROVED_DISTILLATION_SUMMARY.md`: Complete system overview

## ğŸš€ Future Improvements

- BLEU score evaluation integration
- Beam search decoding options
- Multi-GPU training support
- Web API interface development
- Real-time translation service

## ğŸ“„ License

This project is for educational and research purposes.

---

**English-Hungarian Neural Machine Translation with Improved Knowledge Distillation** - Professional, Flexible, and Reliable! ğŸ¯









# è‹±åŒˆç¥ç»æœºå™¨ç¿»è¯‘ä¸æ”¹è¿›çŸ¥è¯†è’¸é¦

ä¸€ä¸ªå®Œæ•´çš„åŸºäº Transformer çš„è‹±è¯­åˆ°åŒˆç‰™åˆ©è¯­ç¥ç»æœºå™¨ç¿»è¯‘å®ç°ï¼Œç»“åˆæ”¹è¿›çš„çŸ¥è¯†è’¸é¦ç³»ç»Ÿï¼Œå®ç°æ¨¡å‹å‹ç¼©ä¸åŠ é€Ÿã€‚

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ª**å…ˆè¿›çš„ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿ**ï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š

* ä½¿ç”¨ Transformer æ¶æ„å°†**è‹±è¯­ç¿»è¯‘ä¸ºåŒˆç‰™åˆ©è¯­**
* ä½¿ç”¨æ”¹è¿›çš„çŸ¥è¯†è’¸é¦ç³»ç»Ÿï¼Œåˆ›å»ºé«˜æ•ˆå‹ç¼©æ¨¡å‹
* æä¾›å¤šç§è®­ç»ƒé…ç½®ä»¥æ»¡è¶³ä¸åŒéœ€æ±‚
* åŒ…å«å®Œæ•´çš„è®­ç»ƒã€æµ‹è¯•å’Œè¯„ä¼°æµç¨‹

## ğŸ“ é¡¹ç›®ç»“æ„

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cmn_hun.py                      # è‹±åŒˆç¿»è¯‘æ•°æ®é¢„å¤„ç†
â”‚   â””â”€â”€ hun-eng/                        # åŒˆç‰™åˆ©è¯­-è‹±è¯­å¹³è¡Œè¯­æ–™
â”œâ”€â”€ model/
â”‚   â””â”€â”€ transformer.py                  # Transformer æ¨¡å‹å®ç°
â”œâ”€â”€ train_cmn_hun_transformer.py        # æ•™å¸ˆæ¨¡å‹è®­ç»ƒè„šæœ¬
â”œâ”€â”€ improved_distillation_trainer.py    # æ”¹è¿›çš„çŸ¥è¯†è’¸é¦è®­ç»ƒè„šæœ¬
â”œâ”€â”€ test_trained_student.py             # å­¦ç”Ÿæ¨¡å‹æµ‹è¯•ä¸è¯„ä¼°
â”œâ”€â”€ inference_cmn_hun_transformer.py    # ç¿»è¯‘æ¨ç†è„šæœ¬
â””â”€â”€ train_process/                      # è®­ç»ƒè¾“å‡ºåŠæ£€æŸ¥ç‚¹
    â”œâ”€â”€ transformer-cmn-hun/            # æ•™å¸ˆæ¨¡å‹æ£€æŸ¥ç‚¹
    â””â”€â”€ distillation-[config]/          # ä¸åŒé…ç½®ä¸‹å­¦ç”Ÿæ¨¡å‹æ£€æŸ¥ç‚¹
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ è®­ç»ƒæ•™å¸ˆæ¨¡å‹

```bash
python train_cmn_hun_transformer.py
```

### 2ï¸âƒ£ è¿›è¡ŒçŸ¥è¯†è’¸é¦è®­ç»ƒ

```bash
# å¯åŠ¨æ”¹è¿›çš„è’¸é¦è®­ç»ƒ
python improved_distillation_trainer.py

# å¯é€‰æ‹©ä»¥ä¸‹ 5 ç§é…ç½®ï¼š
# 1. å¿«é€Ÿæµ‹è¯•ï¼ˆ15 åˆ†é’Ÿï¼‰- éªŒè¯ç”¨
# 2. ç¨³å¥æ¨¡å¼ï¼ˆ2 å°æ—¶ï¼‰- ç¨³å®šè®­ç»ƒ
# 3. å¹³è¡¡æ¨¡å¼ï¼ˆ3 å°æ—¶ï¼‰- æ¨è
# 4. æ¿€è¿›æ¨¡å¼ï¼ˆ5 å°æ—¶ï¼‰- é«˜å‹ç¼©ç‡
# 5. é«˜è´¨é‡æ¨¡å¼ï¼ˆ8 å°æ—¶ï¼‰- æœ€ä½³æ•ˆæœ
```

### 3ï¸âƒ£ æµ‹è¯•å­¦ç”Ÿæ¨¡å‹

```bash
python test_trained_student.py --model_path ./train_process/distillation-[config]/checkpoints/student_best.pt
```

### 4ï¸âƒ£ ç¿»è¯‘æ¨ç†

```bash
python inference_cmn_hun_transformer.py
```

## ğŸ“Š è®­ç»ƒé…ç½®

### å¯ç”¨é…ç½®

| é…ç½®        | è®­ç»ƒæ—¶é•¿  | å‹ç¼©æ¯”  | é€‚ç”¨åœºæ™¯ |
| --------- | ----- | ---- | ---- |
| **å¿«é€Ÿæµ‹è¯•**  | 15 åˆ†é’Ÿ | 1.8x | éªŒè¯ä»£ç  |
| **ç¨³å¥æ¨¡å¼**  | 2 å°æ—¶  | 1.7x | ç¨³å®šè®­ç»ƒ |
| **å¹³è¡¡æ¨¡å¼**  | 3 å°æ—¶  | 2.0x | æ¨è   |
| **æ¿€è¿›æ¨¡å¼**  | 5 å°æ—¶  | 3.0x | é«˜å‹ç¼©  |
| **é«˜è´¨é‡æ¨¡å¼** | 8 å°æ—¶  | 1.8x | æœ€ä½³æ•ˆæœ |

### é¢„æœŸæ€§èƒ½

* **æ¨¡å‹å‹ç¼©**ï¼š1.7x - 3.0xï¼ˆè§†é…ç½®è€Œå®šï¼‰
* **æ¨ç†åŠ é€Ÿ**ï¼šæ¯”æ•™å¸ˆæ¨¡å‹å¿« 1.5x - 3.0x
* **è®­ç»ƒæ—¶é•¿**ï¼š15 åˆ†é’Ÿ - 8 å°æ—¶ï¼ˆæ ¹æ®è´¨é‡è¦æ±‚é€‰æ‹©ï¼‰

## ğŸ› ï¸ æŠ€æœ¯ç‰¹æ€§

### æ”¹è¿›çš„çŸ¥è¯†è’¸é¦

* **å¤šé…ç½®æ”¯æŒ**ï¼š5 ç§ä¸åŒè®­ç»ƒæ¨¡å¼
* **é«˜çº§æŸå¤±å‡½æ•°**ï¼šæ ‡ç­¾å¹³æ»‘ + æ”¹è¿›çš„æ©ç å¤„ç†
* **ä¼˜åŒ–è®­ç»ƒ**ï¼šä½™å¼¦é€€ç«è°ƒåº¦ + æ¢¯åº¦ç´¯ç§¯
* **å®Œæ•´ç›‘æ§**ï¼šé›†æˆ TensorBoard + è¯¦ç»†æ—¥å¿—è®°å½•

### æ¨¡å‹æ¶æ„

* **Transformer ç¼–ç å™¨-è§£ç å™¨ç»“æ„**ï¼šæ ‡å‡†æ¶æ„å¹¶è¿›è¡Œä¼˜åŒ–
* **åŠ¨æ€å‹ç¼©**ï¼šæ”¯æŒ 1.7x - 3.0x çµæ´»å‹ç¼©
* **çµæ´»åºåˆ—é•¿åº¦**ï¼šå¯æ ¹æ®ä¸åŒé…ç½®è‡ªé€‚åº”è°ƒæ•´
* **è¯æ±‡è¡¨**ï¼šè‹±è¯­ä¸åŒˆç‰™åˆ©è¯­åˆ†è¯å™¨å¹¶å¸¦ç¼“å­˜

### è®­ç»ƒä¼˜åŒ–

* **AdamW ä¼˜åŒ–å™¨**ï¼šæ”¯æŒæƒé‡è¡°å‡ä¸é«˜çº§è°ƒåº¦
* **æ¢¯åº¦ç®¡ç†**ï¼šå‰ªè£ + ç´¯ç§¯ç¡®ä¿ç¨³å®š
* **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸æ•æ‰ä¸æ¢å¤æœºåˆ¶
* **è‡ªåŠ¨ä¿å­˜**ï¼šæœ€ä½³æ¨¡å‹è‡ªåŠ¨é€‰æ‹©ä¸æ£€æŸ¥ç‚¹ç®¡ç†

## ğŸ¯ æ ¸å¿ƒæ–‡ä»¶è¯´æ˜

### æ ¸å¿ƒè®­ç»ƒæ–‡ä»¶

* **`train_cmn_hun_transformer.py`**ï¼šè®­ç»ƒæ•™å¸ˆæ¨¡å‹
* **`improved_distillation_trainer.py`**ï¼šè¿›è¡Œæ”¹è¿›çš„çŸ¥è¯†è’¸é¦
* **`test_trained_student.py`**ï¼šæµ‹è¯•å’Œè¯„ä¼°å­¦ç”Ÿæ¨¡å‹

### æ¨¡å‹ä¸æ•°æ®æ–‡ä»¶

* **`model/transformer.py`**ï¼šä¼˜åŒ–çš„ Transformer æ¨¡å‹å®ç°
* **`data/cmn_hun.py`**ï¼šè‹±è¯­-åŒˆç‰™åˆ©è¯­æ•°æ®å¤„ç†
* **`inference_cmn_hun_transformer.py`**ï¼šç¿»è¯‘æ¨ç†æ¥å£

## ğŸ“ˆ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´è®­ç»ƒæµç¨‹

```bash
# 1. è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼ˆå¦‚å°šæœªè®­ç»ƒï¼‰
python train_cmn_hun_transformer.py

# 2. è¿è¡ŒçŸ¥è¯†è’¸é¦
python improved_distillation_trainer.py
# å»ºè®®é€‰æ‹©é…ç½®ï¼š3ï¼ˆå¹³è¡¡æ¨¡å¼ï¼‰

# 3. æµ‹è¯•å­¦ç”Ÿæ¨¡å‹
python test_trained_student.py --model_path ./train_process/distillation-balanced/checkpoints/student_best.pt

# 4. è¿›è¡Œç¿»è¯‘æ¨ç†
python inference_cmn_hun_transformer.py
```

### äº¤äº’å¼ç¿»è¯‘

```bash
# å¯åŠ¨äº¤äº’å¼ç¿»è¯‘å™¨
python inference_cmn_hun_transformer.py

# è¾“å…¥ç¤ºä¾‹ï¼š
en:Hello world
hun:JÃ³ reggelt!
```

## ğŸ”§ ä¾èµ–ç¯å¢ƒ

* Python 3.8+
* PyTorch 1.12+
* torchtext
* tqdm
* tensorboard
* numpy

## ğŸ‰ é¡¹ç›®äº®ç‚¹

æœ¬é¡¹ç›®æˆåŠŸå±•ç¤ºï¼š

* **å…ˆè¿›çš„çŸ¥è¯†è’¸é¦æ–¹æ³•**ï¼šæ”¯æŒå¤šç§è®­ç»ƒé…ç½®ä»¥æ»¡è¶³ä¸åŒéœ€æ±‚
* **çµæ´»å‹ç¼©èƒ½åŠ›**ï¼šæ”¯æŒ 1.7x - 3.0x æ¨¡å‹å‹ç¼©
* **æ”¹è¿›è®­ç»ƒæµç¨‹**ï¼šç¨³å®šã€å¯é çš„è’¸é¦è¿‡ç¨‹
* **å®Œæ•´ç®¡çº¿**ï¼šæ¶µç›–æ•™å¸ˆè®­ç»ƒåˆ°å­¦ç”Ÿè¯„ä¼°å…¨æµç¨‹
* **ç”Ÿäº§å¯ç”¨**ï¼šä¸“ä¸šçº§è®­ç»ƒä¸æµ‹è¯•å·¥å…·

## ğŸ“– æ–‡æ¡£

* `TRAINING_CONFIGURATIONS_GUIDE.md`ï¼šè¯¦ç»†é…ç½®ä½¿ç”¨æŒ‡å—
* `IMPROVED_DISTILLATION_SUMMARY.md`ï¼šå®Œæ•´ç³»ç»Ÿç»¼è¿°

## ğŸš€ åç»­æ”¹è¿›è®¡åˆ’

* é›†æˆ BLEU åˆ†æ•°è‡ªåŠ¨è¯„ä¼°
* å¢åŠ  Beam Search è§£ç é€‰é¡¹
* æ”¯æŒå¤š GPU è®­ç»ƒ
* å¼€å‘ Web API æ¥å£
* éƒ¨ç½²å®æ—¶ç¿»è¯‘æœåŠ¡

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›æ•™å­¦ä¸ç ”ç©¶ç”¨é€”ã€‚

---

**è‹±åŒˆç¥ç»æœºå™¨ç¿»è¯‘ + æ”¹è¿›çŸ¥è¯†è’¸é¦**
ä¸“ä¸šã€çµæ´»ã€å¯é ï¼ŒåŠ©ä½ å¿«é€Ÿæ­å»ºé«˜è´¨é‡ç¿»è¯‘ç³»ç»Ÿï¼ ğŸ¯

---

å¦‚æœéœ€è¦ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ ç”Ÿæˆè¯¥é¡¹ç›®çš„å®Œæ•´å¯æ‰§è¡Œ**è®­ç»ƒã€æ¨ç†ä¸å¯è§†åŒ–è„šæœ¬**ä»¥ä¾¿ç›´æ¥è·‘èµ·æ¥ç»ƒä¹ æ·±åº¦å­¦ä¹ å®Œæ•´æµç¨‹ï¼Œå‘Šè¯‰æˆ‘å³å¯ã€‚

