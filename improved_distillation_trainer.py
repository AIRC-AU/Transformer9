#!/usr/bin/env python3
"""
Improved Knowledge Distillation Trainer for Transformer Models
é‡æ–°è®¾è®¡çš„çŸ¥è¯†è’¸é¦è®­ç»ƒç³»ç»Ÿ - å¤šæ–¹æ¡ˆå¯é€‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from torch.nn.functional import pad
import argparse
import time
from pathlib import Path
from tqdm import tqdm
import math

from data.cmn_hun import TranslationDataset
from model.transformer import TranslationModel


class ImprovedDistillationLoss(nn.Module):
    """æ”¹è¿›çš„è’¸é¦æŸå¤±å‡½æ•°"""
    def __init__(self, temperature=3.0, alpha=0.7, beta=0.3, label_smoothing=0.1):
        super(ImprovedDistillationLoss, self).__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.beta = beta
        self.kl_div = nn.KLDivLoss(reduction="batchmean")
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=2, label_smoothing=label_smoothing)
        
    def forward(self, student_logits, teacher_logits, hard_targets):
        # Reshape for loss calculation
        batch_size, seq_len, vocab_size = student_logits.shape
        student_logits_flat = student_logits.reshape(-1, vocab_size)
        teacher_logits_flat = teacher_logits.reshape(-1, vocab_size)
        hard_targets_flat = hard_targets.reshape(-1)
        
        # Create mask for valid positions (not padding)
        mask = (hard_targets_flat != 2)
        
        if mask.sum() == 0:  # All padding
            return torch.tensor(0.0, device=student_logits.device)
        
        # Apply mask
        student_masked = student_logits_flat[mask]
        teacher_masked = teacher_logits_flat[mask]
        targets_masked = hard_targets_flat[mask]
        
        # Soft targets from teacher (with temperature)
        student_soft = F.log_softmax(student_masked / self.temperature, dim=-1)
        teacher_soft = F.softmax(teacher_masked / self.temperature, dim=-1)
        
        # Distillation loss (KL divergence)
        distillation_loss = self.kl_div(student_soft, teacher_soft) * (self.temperature ** 2)
        
        # Hard target loss (cross entropy with label smoothing)
        hard_loss = self.ce_loss(student_masked, targets_masked)
        
        # Combined loss
        total_loss = self.alpha * distillation_loss + self.beta * hard_loss
        
        return total_loss, distillation_loss, hard_loss


class TrainingConfig:
    """è®­ç»ƒé…ç½®åŸºç±»"""
    def __init__(self):
        self.name = "base"
        self.epochs = 20
        self.batch_size = 32
        self.learning_rate = 0.001
        self.max_seq_length = 24
        self.student_reduction_factor = 0.5
        self.temperature = 3.0
        self.alpha = 0.7
        self.beta = 0.3
        self.warmup_steps = 1000
        self.weight_decay = 0.01
        self.gradient_clip = 1.0
        self.accumulation_steps = 1
        self.save_every = 5
        self.eval_every = 2
        self.use_scheduler = True
        self.label_smoothing = 0.1
        self.data_subset_ratio = 1.0  # Use full dataset
        
    def get_description(self):
        return f"{self.name} - {self.epochs}è½®, æ‰¹æ¬¡{self.batch_size}, å­¦ä¹ ç‡{self.learning_rate}"


class QuickTestConfig(TrainingConfig):
    """å¿«é€Ÿæµ‹è¯•é…ç½® - éªŒè¯ä»£ç æ­£ç¡®æ€§"""
    def __init__(self):
        super().__init__()
        self.name = "quick_test"
        self.epochs = 3
        self.batch_size = 64
        self.learning_rate = 0.002
        self.max_seq_length = 16
        self.student_reduction_factor = 0.6
        self.data_subset_ratio = 0.1  # Only 10% of data
        self.save_every = 1
        self.eval_every = 1
        
    def get_description(self):
        return "å¿«é€Ÿæµ‹è¯• - 3è½®, 10%æ•°æ®, çº¦15åˆ†é’Ÿ"


class ConservativeConfig(TrainingConfig):
    """ä¿å®ˆé…ç½® - ç¨³å®šè®­ç»ƒ"""
    def __init__(self):
        super().__init__()
        self.name = "conservative"
        self.epochs = 30
        self.batch_size = 16
        self.learning_rate = 0.0005
        self.max_seq_length = 28
        self.student_reduction_factor = 0.6
        self.temperature = 4.0
        self.alpha = 0.8
        self.beta = 0.2
        self.warmup_steps = 2000
        
    def get_description(self):
        return "ä¿å®ˆè®­ç»ƒ - 30è½®, 60%å‹ç¼©, çº¦2å°æ—¶"


class BalancedConfig(TrainingConfig):
    """å¹³è¡¡é…ç½® - æ€§èƒ½ä¸æ•ˆç‡å¹³è¡¡"""
    def __init__(self):
        super().__init__()
        self.name = "balanced"
        self.epochs = 50
        self.batch_size = 32
        self.learning_rate = 0.001
        self.max_seq_length = 24
        self.student_reduction_factor = 0.5
        self.temperature = 3.5
        self.alpha = 0.75
        self.beta = 0.25
        self.accumulation_steps = 2
        
    def get_description(self):
        return "å¹³è¡¡è®­ç»ƒ - 50è½®, 50%å‹ç¼©, çº¦3å°æ—¶"


class AggressiveConfig(TrainingConfig):
    """æ¿€è¿›é…ç½® - é«˜å‹ç¼©æ¯”"""
    def __init__(self):
        super().__init__()
        self.name = "aggressive"
        self.epochs = 80
        self.batch_size = 24
        self.learning_rate = 0.0008
        self.max_seq_length = 24
        self.student_reduction_factor = 0.35
        self.temperature = 5.0
        self.alpha = 0.85
        self.beta = 0.15
        self.warmup_steps = 3000
        self.accumulation_steps = 3
        
    def get_description(self):
        return "æ¿€è¿›è®­ç»ƒ - 80è½®, 35%å‹ç¼©, çº¦5å°æ—¶"


class HighQualityConfig(TrainingConfig):
    """é«˜è´¨é‡é…ç½® - æœ€ä½³æ•ˆæœ"""
    def __init__(self):
        super().__init__()
        self.name = "high_quality"
        self.epochs = 100
        self.batch_size = 16
        self.learning_rate = 0.0003
        self.max_seq_length = 32
        self.student_reduction_factor = 0.55
        self.temperature = 4.5
        self.alpha = 0.8
        self.beta = 0.2
        self.warmup_steps = 4000
        self.accumulation_steps = 4
        self.weight_decay = 0.02
        
    def get_description(self):
        return "é«˜è´¨é‡è®­ç»ƒ - 100è½®, 55%å‹ç¼©, çº¦8å°æ—¶"


def get_config(config_name):
    """è·å–é…ç½®"""
    configs = {
        'quick': QuickTestConfig(),
        'conservative': ConservativeConfig(),
        'balanced': BalancedConfig(),
        'aggressive': AggressiveConfig(),
        'quality': HighQualityConfig()
    }
    return configs.get(config_name, BalancedConfig())


def create_student_model(teacher_model, config, dataset):
    """åˆ›å»ºå­¦ç”Ÿæ¨¡å‹"""
    teacher_d_model = teacher_model.src_embedding.embedding_dim
    student_d_model = int(teacher_d_model * config.student_reduction_factor)
    
    # Ensure d_model is divisible by num_heads (8)
    student_d_model = max(8, (student_d_model // 8) * 8)
    
    print(f"Teacher d_model: {teacher_d_model}")
    print(f"Student d_model: {student_d_model} ({config.student_reduction_factor:.1%} of teacher)")
    
    student_model = TranslationModel(
        d_model=student_d_model,
        src_vocab=dataset.zh_vocab,
        tgt_vocab=dataset.hun_vocab,
        max_seq_length=config.max_seq_length,
        device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    )
    
    return student_model


def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, min_lr_ratio=0.1):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        cosine_decay = 0.5 * (1.0 + math.cos(math.pi * progress))
        return min_lr_ratio + (1.0 - min_lr_ratio) * cosine_decay
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


def collate_fn(batch, max_seq_length, device):
    """æ•°æ®æ•´ç†å‡½æ•°"""
    bs_id, eos_id, pad_id = 0, 1, 2
    src_list, tgt_list = [], []

    for _src, _tgt in batch:
        # Limit sequence length
        src_tensor = _src[:max_seq_length - 2].clone().detach()
        tgt_tensor = _tgt[:max_seq_length - 2].clone().detach()

        # Add special tokens
        processed_src = torch.cat([
            torch.tensor([bs_id], dtype=torch.int64),
            src_tensor,
            torch.tensor([eos_id], dtype=torch.int64)
        ])

        processed_tgt = torch.cat([
            torch.tensor([bs_id], dtype=torch.int64),
            tgt_tensor,
            torch.tensor([eos_id], dtype=torch.int64)
        ])

        # Pad to max length
        src_list.append(pad(
            processed_src,
            (0, max_seq_length - len(processed_src)),
            value=pad_id
        ))

        tgt_list.append(pad(
            processed_tgt,
            (0, max_seq_length - len(processed_tgt)),
            value=pad_id
        ))

    src = torch.stack(src_list).to(device)
    tgt = torch.stack(tgt_list).to(device)
    tgt_y = tgt[:, 1:]  # Target output (shifted)
    tgt = tgt[:, :-1]   # Target input

    return src, tgt, tgt_y


def evaluate_model(model, data_loader, criterion, device, max_batches=50):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    total_batches = 0
    
    with torch.no_grad():
        for batch_idx, (src, tgt, tgt_y) in enumerate(data_loader):
            if batch_idx >= max_batches:
                break
                
            try:
                output = model(src, tgt)
                logits = model.predictor(output)
                
                # Calculate loss (only hard target loss for evaluation)
                logits_flat = logits.reshape(-1, logits.size(-1))
                targets_flat = tgt_y.reshape(-1)
                mask = (targets_flat != 2)
                
                if mask.sum() > 0:
                    loss = F.cross_entropy(logits_flat[mask], targets_flat[mask])
                    total_loss += loss.item()
                    total_batches += 1
                    
            except Exception as e:
                print(f"Evaluation error at batch {batch_idx}: {e}")
                continue
    
    model.train()
    return total_loss / max(total_batches, 1)


def train_student_model(config):
    """è®­ç»ƒå­¦ç”Ÿæ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ: {config.get_description()}")
    print(f"{'='*60}")
    
    # Setup
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # Paths
    teacher_model_path = "./train_process/transformer-cmn-hun/transformer_checkpoints/best.pt"
    output_dir = Path(f"./train_process/distillation-{config.name}")
    model_dir = output_dir / "checkpoints"
    log_dir = output_dir / "logs"
    
    # Create directories
    output_dir.mkdir(parents=True, exist_ok=True)
    model_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # Load dataset
    print("åŠ è½½æ•°æ®é›†...")
    dataset = TranslationDataset("data/hun-eng/hun.txt")
    
    # Create data subset if needed
    if config.data_subset_ratio < 1.0:
        subset_size = int(len(dataset) * config.data_subset_ratio)
        indices = list(range(subset_size))
        dataset = Subset(dataset, indices)
        print(f"ä½¿ç”¨æ•°æ®å­é›†: {subset_size} æ ·æœ¬")
    
    # Create data loader
    train_loader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=lambda batch: collate_fn(batch, config.max_seq_length, device),
        num_workers=0,
        pin_memory=False  # Disable pin_memory to avoid CUDA tensor pinning issues
    )
    
    print(f"æ•°æ®åŠ è½½å®Œæˆ: {len(dataset)} æ ·æœ¬, {len(train_loader)} æ‰¹æ¬¡")
    
    # Load teacher model
    print("åŠ è½½æ•™å¸ˆæ¨¡å‹...")
    if not Path(teacher_model_path).exists():
        raise FileNotFoundError(f"æ•™å¸ˆæ¨¡å‹ä¸å­˜åœ¨: {teacher_model_path}")
    
    teacher_model = torch.load(teacher_model_path, map_location=device)
    teacher_model.to(device)
    teacher_model.eval()
    
    # Create student model
    print("åˆ›å»ºå­¦ç”Ÿæ¨¡å‹...")
    student_model = create_student_model(teacher_model, config, dataset.dataset if hasattr(dataset, 'dataset') else dataset)
    student_model.to(device)
    
    # Model info
    teacher_params = sum(p.numel() for p in teacher_model.parameters())
    student_params = sum(p.numel() for p in student_model.parameters())
    compression_ratio = teacher_params / student_params
    
    print(f"\næ¨¡å‹ä¿¡æ¯:")
    print(f"æ•™å¸ˆæ¨¡å‹å‚æ•°: {teacher_params:,}")
    print(f"å­¦ç”Ÿæ¨¡å‹å‚æ•°: {student_params:,}")
    print(f"å‹ç¼©æ¯”: {compression_ratio:.2f}x")
    print(f"å‚æ•°å‡å°‘: {(1 - student_params/teacher_params)*100:.1f}%")
    
    return student_model, teacher_model, train_loader, device, model_dir, log_dir


def run_training_loop(student_model, teacher_model, train_loader, config, device, model_dir, log_dir):
    """å®Œæ•´çš„è®­ç»ƒå¾ªç¯"""

    # Setup training components
    criterion = ImprovedDistillationLoss(
        temperature=config.temperature,
        alpha=config.alpha,
        beta=config.beta,
        label_smoothing=config.label_smoothing
    )

    optimizer = torch.optim.AdamW(
        student_model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay,
        betas=(0.9, 0.98),
        eps=1e-9
    )

    # Learning rate scheduler
    total_steps = len(train_loader) * config.epochs // config.accumulation_steps
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        config.warmup_steps,
        total_steps
    ) if config.use_scheduler else None

    # TensorBoard logging
    writer = SummaryWriter(log_dir)

    # Training state
    best_loss = float('inf')
    global_step = 0
    start_time = time.time()

    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"æ€»æ­¥æ•°: {total_steps}, é¢„è®¡æ—¶é—´: {config.get_description().split('çº¦')[-1] if 'çº¦' in config.get_description() else 'æœªçŸ¥'}")

    for epoch in range(config.epochs):
        student_model.train()
        epoch_loss = 0
        epoch_distill_loss = 0
        epoch_hard_loss = 0

        # Progress bar
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.epochs}")

        for batch_idx, (src, tgt, tgt_y) in enumerate(pbar):
            try:
                # Teacher forward pass (no gradients)
                with torch.no_grad():
                    teacher_output = teacher_model(src, tgt)
                    teacher_logits = teacher_model.predictor(teacher_output)

                # Student forward pass
                student_output = student_model(src, tgt)
                student_logits = student_model.predictor(student_output)

                # Calculate loss
                total_loss, distill_loss, hard_loss = criterion(
                    student_logits, teacher_logits, tgt_y
                )

                # Scale loss for gradient accumulation
                total_loss = total_loss / config.accumulation_steps

                # Backward pass
                total_loss.backward()

                # Update weights
                if (batch_idx + 1) % config.accumulation_steps == 0:
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(student_model.parameters(), config.gradient_clip)

                    optimizer.step()
                    optimizer.zero_grad()

                    if scheduler:
                        scheduler.step()

                    global_step += 1

                # Accumulate losses
                epoch_loss += total_loss.item() * config.accumulation_steps
                epoch_distill_loss += distill_loss.item()
                epoch_hard_loss += hard_loss.item()

                # Update progress bar
                current_lr = scheduler.get_last_lr()[0] if scheduler else config.learning_rate
                pbar.set_postfix({
                    'Loss': f'{total_loss.item() * config.accumulation_steps:.4f}',
                    'LR': f'{current_lr:.6f}'
                })

                # Log to TensorBoard
                if global_step % 100 == 0:
                    writer.add_scalar('Loss/Total', total_loss.item() * config.accumulation_steps, global_step)
                    writer.add_scalar('Loss/Distillation', distill_loss.item(), global_step)
                    writer.add_scalar('Loss/Hard', hard_loss.item(), global_step)
                    writer.add_scalar('Learning_Rate', current_lr, global_step)

            except Exception as e:
                print(f"\nâŒ è®­ç»ƒé”™è¯¯ (Epoch {epoch+1}, Batch {batch_idx}): {e}")
                continue

        # Epoch statistics
        avg_loss = epoch_loss / len(train_loader)
        avg_distill = epoch_distill_loss / len(train_loader)
        avg_hard = epoch_hard_loss / len(train_loader)

        elapsed_time = time.time() - start_time
        eta = elapsed_time / (epoch + 1) * (config.epochs - epoch - 1)

        print(f"\nEpoch {epoch+1}/{config.epochs} å®Œæˆ:")
        print(f"  æ€»æŸå¤±: {avg_loss:.4f}")
        print(f"  è’¸é¦æŸå¤±: {avg_distill:.4f}")
        print(f"  ç¡¬ç›®æ ‡æŸå¤±: {avg_hard:.4f}")
        print(f"  å·²ç”¨æ—¶é—´: {elapsed_time/60:.1f}åˆ†é’Ÿ")
        print(f"  é¢„è®¡å‰©ä½™: {eta/60:.1f}åˆ†é’Ÿ")

        # Save model
        if (epoch + 1) % config.save_every == 0 or avg_loss < best_loss:
            if avg_loss < best_loss:
                best_loss = avg_loss
                torch.save(student_model, model_dir / 'student_best.pt')
                print(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (æŸå¤±: {avg_loss:.4f})")

            torch.save(student_model, model_dir / f'student_epoch_{epoch+1}.pt')
            print(f"  ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: epoch_{epoch+1}")

        # Evaluation
        if (epoch + 1) % config.eval_every == 0:
            print("  ğŸ” è¯„ä¼°ä¸­...")
            eval_loss = evaluate_model(student_model, train_loader, criterion, device)
            print(f"  è¯„ä¼°æŸå¤±: {eval_loss:.4f}")
            writer.add_scalar('Eval/Loss', eval_loss, epoch)

    # Training completed
    total_time = time.time() - start_time
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"æ€»ç”¨æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
    print(f"æœ€ä½³æŸå¤±: {best_loss:.4f}")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {model_dir}")

    writer.close()
    return student_model


if __name__ == "__main__":
    # Show available configurations
    print("ğŸ¯ æ”¹è¿›çš„çŸ¥è¯†è’¸é¦è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    print("å¯ç”¨çš„è®­ç»ƒé…ç½®:")
    print("-" * 60)

    configs = ['quick', 'conservative', 'balanced', 'aggressive', 'quality']

    for i, config_name in enumerate(configs, 1):
        config = get_config(config_name)
        print(f"{i}. {config_name:12} - {config.get_description()}")

    print("-" * 60)
    print("è¯·é€‰æ‹©è®­ç»ƒé…ç½® (è¾“å…¥æ•°å­— 1-5):")

    try:
        choice = int(input().strip())
        if 1 <= choice <= 5:
            config_name = configs[choice - 1]
            config = get_config(config_name)
            print(f"\nâœ… å·²é€‰æ‹©: {config.get_description()}")

            # Confirm training
            print(f"\nâš ï¸  å³å°†å¼€å§‹è®­ç»ƒï¼Œé¢„è®¡ç”¨æ—¶: {config.get_description().split('çº¦')[-1] if 'çº¦' in config.get_description() else 'æœªçŸ¥'}")
            confirm = input("ç¡®è®¤å¼€å§‹è®­ç»ƒ? (y/N): ").strip().lower()

            if confirm in ['y', 'yes']:
                # Initialize and run training
                student_model, teacher_model, train_loader, device, model_dir, log_dir = train_student_model(config)

                # Run complete training
                trained_model = run_training_loop(
                    student_model, teacher_model, train_loader,
                    config, device, model_dir, log_dir
                )

                print(f"\nğŸ¯ è®­ç»ƒå®Œæˆ! å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æµ‹è¯•æ¨¡å‹:")
                print(f"python test_trained_student.py --model_path {model_dir}/student_best.pt")

            else:
                print("âŒ è®­ç»ƒå·²å–æ¶ˆ")
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-5 ä¹‹é—´çš„æ•°å­—")

    except ValueError:
        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
