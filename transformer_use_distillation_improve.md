# Transformerä½¿ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯æå‡æœºå™¨ç¿»è¯‘æ•ˆæœ

## ğŸ“‹ ç›®å½•
1. [çŸ¥è¯†è’¸é¦åŸç†](#1-çŸ¥è¯†è’¸é¦åŸç†)
2. [Transformeræ¶æ„åˆ†æ](#2-transformeræ¶æ„åˆ†æ)
3. [è’¸é¦æŸå¤±å‡½æ•°è®¾è®¡](#3-è’¸é¦æŸå¤±å‡½æ•°è®¾è®¡)
4. [å®æ–½æ–¹æ¡ˆä¸æµç¨‹](#4-å®æ–½æ–¹æ¡ˆä¸æµç¨‹)
5. [ä»£ç å®ç°](#5-ä»£ç å®ç°)
6. [ä¼˜åŒ–ç­–ç•¥](#6-ä¼˜åŒ–ç­–ç•¥)
7. [å®éªŒç»“æœåˆ†æ](#7-å®éªŒç»“æœåˆ†æ)

---

## 1. çŸ¥è¯†è’¸é¦åŸç†

### 1.1 åŸºæœ¬æ¦‚å¿µ

çŸ¥è¯†è’¸é¦(Knowledge Distillation)æ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªå°å‹çš„å­¦ç”Ÿæ¨¡å‹æ¥æ¨¡ä»¿å¤§å‹æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºã€‚åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œè¿™ç§æŠ€æœ¯å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹çš„æ•ˆç‡è€Œä¸å¤§å¹…æŸå¤±ç¿»è¯‘è´¨é‡ã€‚

### 1.2 æ ¸å¿ƒæ€æƒ³

**æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶**ï¼š
- **æ•™å¸ˆæ¨¡å‹(Teacher Model)**: å¤§å‹ã€é«˜æ€§èƒ½çš„Transformeræ¨¡å‹
- **å­¦ç”Ÿæ¨¡å‹(Student Model)**: å°å‹ã€é«˜æ•ˆçš„Transformeræ¨¡å‹
- **çŸ¥è¯†ä¼ é€’**: é€šè¿‡è½¯æ ‡ç­¾(Soft Labels)ä¼ é€’æ•™å¸ˆæ¨¡å‹çš„"æš—çŸ¥è¯†"

### 1.3 ç†è®ºåŸºç¡€

ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ ä½¿ç”¨ç¡¬æ ‡ç­¾(Hard Labels)ï¼Œå³one-hotç¼–ç çš„çœŸå®æ ‡ç­¾ã€‚çŸ¥è¯†è’¸é¦å¼•å…¥äº†è½¯æ ‡ç­¾æ¦‚å¿µï¼š

**ç¡¬æ ‡ç­¾**: $y_{hard} = [0, 0, 1, 0, ..., 0]$ (one-hot)

**è½¯æ ‡ç­¾**: $y_{soft} = [0.1, 0.05, 0.7, 0.1, ..., 0.05]$ (æ¦‚ç‡åˆ†å¸ƒ)

è½¯æ ‡ç­¾åŒ…å«äº†æ¨¡å‹å¯¹ä¸åŒè¾“å‡ºé€‰é¡¹çš„ç½®ä¿¡åº¦ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯è¢«ç§°ä¸º"æš—çŸ¥è¯†"(Dark Knowledge)ã€‚

---

## 2. Transformeræ¶æ„åˆ†æ

### 2.1 æ ‡å‡†Transformerç»“æ„

```
è¾“å…¥åºåˆ— â†’ ç¼–ç å™¨ â†’ è§£ç å™¨ â†’ è¾“å‡ºåºåˆ—
    â†“         â†“        â†“         â†“
  Embedding  Multi-Head  Multi-Head  Linear
             Attention   Attention   Projection
```

### 2.2 æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹è®¾è®¡

**æ•™å¸ˆæ¨¡å‹ç‰¹å¾**:
- æ›´æ·±çš„ç½‘ç»œå±‚æ•° (å¦‚12-24å±‚)
- æ›´å¤§çš„éšè—ç»´åº¦ (å¦‚512-1024ç»´)
- æ›´å¤šçš„æ³¨æ„åŠ›å¤´ (å¦‚8-16ä¸ª)
- æ›´å¤§çš„å‰é¦ˆç½‘ç»œç»´åº¦

**å­¦ç”Ÿæ¨¡å‹ç‰¹å¾**:
- è¾ƒå°‘çš„ç½‘ç»œå±‚æ•° (å¦‚6-12å±‚)
- è¾ƒå°çš„éšè—ç»´åº¦ (å¦‚256-512ç»´)
- è¾ƒå°‘çš„æ³¨æ„åŠ›å¤´ (å¦‚4-8ä¸ª)
- è¾ƒå°çš„å‰é¦ˆç½‘ç»œç»´åº¦

### 2.3 æ¨¡å‹å‹ç¼©æ¯”ä¾‹

å…¸å‹çš„å‹ç¼©ç­–ç•¥ï¼š
- **æ·±åº¦å‹ç¼©**: å±‚æ•°å‡å°‘50-75%
- **å®½åº¦å‹ç¼©**: éšè—ç»´åº¦å‡å°‘25-50%
- **æ³¨æ„åŠ›å¤´å‹ç¼©**: æ³¨æ„åŠ›å¤´æ•°é‡å‡å°‘50%

---

## 3. è’¸é¦æŸå¤±å‡½æ•°è®¾è®¡

### 3.1 æ¸©åº¦ç¼©æ”¾Softmax

æ ‡å‡†softmaxå‡½æ•°ï¼š
$$P_i = \frac{e^{z_i}}{\sum_{j=1}^{N} e^{z_j}}$$

æ¸©åº¦ç¼©æ”¾softmaxï¼š
$$P_i(T) = \frac{e^{z_i/T}}{\sum_{j=1}^{N} e^{z_j/T}}$$

å…¶ä¸­ï¼š
- $z_i$ æ˜¯ç¬¬iä¸ªç±»åˆ«çš„logitå€¼
- $T$ æ˜¯æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶æ¦‚ç‡åˆ†å¸ƒçš„"è½¯åº¦"
- $T > 1$ æ—¶ï¼Œåˆ†å¸ƒæ›´å¹³æ»‘ï¼ŒåŒ…å«æ›´å¤šæš—çŸ¥è¯†
- $T = 1$ æ—¶ï¼Œé€€åŒ–ä¸ºæ ‡å‡†softmax

### 3.2 è’¸é¦æŸå¤±å‡½æ•°

å®Œæ•´çš„è’¸é¦æŸå¤±ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š

$$L_{total} = \alpha \cdot L_{distill} + \beta \cdot L_{hard}$$

**è’¸é¦æŸå¤± (Distillation Loss)**:
$$L_{distill} = T^2 \cdot KL(P_{student}(T), P_{teacher}(T))$$

**ç¡¬ç›®æ ‡æŸå¤± (Hard Target Loss)**:
$$L_{hard} = CrossEntropy(P_{student}(T=1), y_{true})$$

å…¶ä¸­ï¼š
- $\alpha$ å’Œ $\beta$ æ˜¯æƒé‡ç³»æ•°ï¼Œé€šå¸¸ $\alpha + \beta = 1$
- $KL$ æ˜¯KLæ•£åº¦æŸå¤±
- $T^2$ æ˜¯æ¸©åº¦å¹³æ–¹é¡¹ï¼Œç”¨äºå¹³è¡¡æ¢¯åº¦å°ºåº¦

### 3.3 KLæ•£åº¦è¯¦ç»†æ¨å¯¼

KLæ•£åº¦å®šä¹‰ï¼š
$$KL(P||Q) = \sum_{i=1}^{N} P_i \log \frac{P_i}{Q_i}$$

åœ¨è’¸é¦ä¸­ï¼š
$$L_{distill} = \sum_{i=1}^{N} P_{teacher,i}(T) \log \frac{P_{teacher,i}(T)}{P_{student,i}(T)}$$

æ¢¯åº¦è®¡ç®—ï¼š
$$\frac{\partial L_{distill}}{\partial z_{student,i}} = \frac{1}{T}(P_{student,i}(T) - P_{teacher,i}(T))$$

### 3.4 æ ‡ç­¾å¹³æ»‘

ä¸ºäº†è¿›ä¸€æ­¥æå‡æ•ˆæœï¼Œå¯ä»¥åœ¨ç¡¬ç›®æ ‡æŸå¤±ä¸­åŠ å…¥æ ‡ç­¾å¹³æ»‘ï¼š

$$y_{smooth} = (1-\epsilon) \cdot y_{true} + \frac{\epsilon}{N}$$

å…¶ä¸­ $\epsilon$ æ˜¯å¹³æ»‘å‚æ•°ï¼Œé€šå¸¸å–0.1ã€‚

---

## 4. å®æ–½æ–¹æ¡ˆä¸æµç¨‹

### 4.1 æ•´ä½“æµç¨‹å›¾

```mermaid
graph TD
    A[ğŸ“Š å‡†å¤‡æ•°æ®é›†<br/>â€¢ å¹³è¡Œè¯­æ–™æ”¶é›†<br/>â€¢ æ•°æ®é¢„å¤„ç†<br/>â€¢ è¯æ±‡è¡¨æ„å»º] --> B[ğŸ“ è®­ç»ƒæ•™å¸ˆæ¨¡å‹<br/>â€¢ å¤§å‹Transformer<br/>â€¢ å……åˆ†è®­ç»ƒè‡³æ”¶æ•›<br/>â€¢ ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹]

    B --> C[ğŸ—ï¸ è®¾è®¡å­¦ç”Ÿæ¨¡å‹æ¶æ„<br/>â€¢ ç¡®å®šå‹ç¼©æ¯”ä¾‹<br/>â€¢ å±‚æ•°/ç»´åº¦è®¾è®¡<br/>â€¢ æ³¨æ„åŠ›å¤´é…ç½®]

    C --> D[âš™ï¸ åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹<br/>â€¢ å‚æ•°åˆå§‹åŒ–<br/>â€¢ æƒé‡é¢„è®¾<br/>â€¢ æ¨¡å‹éªŒè¯]

    D --> E[ğŸ”¥ çŸ¥è¯†è’¸é¦è®­ç»ƒ<br/>â€¢ æ¸©åº¦ç¼©æ”¾Softmax<br/>â€¢ è”åˆæŸå¤±å‡½æ•°<br/>â€¢ æ¢¯åº¦ä¼˜åŒ–]

    E --> F[ğŸ“ˆ æ¨¡å‹è¯„ä¼°<br/>â€¢ BLEUåˆ†æ•°è®¡ç®—<br/>â€¢ æ¨ç†é€Ÿåº¦æµ‹è¯•<br/>â€¢ å†…å­˜å ç”¨åˆ†æ]

    F --> G{ğŸ¯ æ•ˆæœæ»¡æ„?<br/>è´¨é‡ vs æ•ˆç‡}

    G -->|âŒ å¦| H[ğŸ”§ è°ƒæ•´è¶…å‚æ•°<br/>â€¢ æ¸©åº¦å‚æ•°T<br/>â€¢ æƒé‡æ¯”ä¾‹Î±/Î²<br/>â€¢ å­¦ä¹ ç‡è°ƒæ•´]

    H --> E

    G -->|âœ… æ˜¯| I[ğŸš€ æ¨¡å‹éƒ¨ç½²<br/>â€¢ ç”Ÿäº§ç¯å¢ƒ<br/>â€¢ æ€§èƒ½ç›‘æ§<br/>â€¢ æŒç»­ä¼˜åŒ–]

    style A fill:#e1f5fe,color:#000
    style B fill:#f3e5f5,color:#000
    style C fill:#e8f5e8,color:#000
    style D fill:#fff3e0,color:#000
    style E fill:#ffebee,color:#000
    style F fill:#f1f8e9,color:#000
    style G fill:#fce4ec,color:#000
    style H fill:#e0f2f1,color:#000
    style I fill:#e8eaf6,color:#000
```

### 4.2 è¯¦ç»†æŠ€æœ¯æµç¨‹å›¾

```mermaid
graph LR
    subgraph "ğŸ“ æ•™å¸ˆæ¨¡å‹"
        T1[è¾“å…¥åºåˆ—] --> T2[Embedding<br/>d_model=512]
        T2 --> T3[12å±‚Transformer<br/>8ä¸ªæ³¨æ„åŠ›å¤´]
        T3 --> T4[è¾“å‡ºLogits<br/>vocab_size]
    end

    subgraph "ğŸ‘¨â€ğŸ“ å­¦ç”Ÿæ¨¡å‹"
        S1[è¾“å…¥åºåˆ—] --> S2[Embedding<br/>d_model=256]
        S2 --> S3[6å±‚Transformer<br/>4ä¸ªæ³¨æ„åŠ›å¤´]
        S3 --> S4[è¾“å‡ºLogits<br/>vocab_size]
    end

    subgraph "ğŸ”¥ è’¸é¦è¿‡ç¨‹"
        T4 --> D1[æ¸©åº¦ç¼©æ”¾<br/>T=4.0]
        S4 --> D2[æ¸©åº¦ç¼©æ”¾<br/>T=4.0]
        D1 --> D3[KLæ•£åº¦æŸå¤±<br/>Î±=0.8]
        D2 --> D3
        S4 --> D4[äº¤å‰ç†µæŸå¤±<br/>Î²=0.2]
        D3 --> D5[æ€»æŸå¤±]
        D4 --> D5
    end

    style T2 fill:#f3e5f5,color:#000
    style T3 fill:#f3e5f5,color:#000
    style T4 fill:#f3e5f5,color:#000
    style S2 fill:#e8f5e8,color:#000
    style S3 fill:#e8f5e8,color:#000
    style S4 fill:#e8f5e8,color:#000
    style D1 fill:#ffebee,color:#000
    style D2 fill:#ffebee,color:#000
    style D3 fill:#ffebee,color:#000
    style D4 fill:#ffebee,color:#000
    style D5 fill:#ffebee,color:#000
```

### 4.2 è¯¦ç»†å®æ–½æ­¥éª¤

#### æ­¥éª¤1: æ•°æ®å‡†å¤‡
- æ”¶é›†é«˜è´¨é‡çš„å¹³è¡Œè¯­æ–™
- æ•°æ®é¢„å¤„ç†å’Œåˆ†è¯
- æ„å»ºè¯æ±‡è¡¨
- æ•°æ®å¢å¼º(å¯é€‰)

#### æ­¥éª¤2: æ•™å¸ˆæ¨¡å‹è®­ç»ƒ
- ä½¿ç”¨æ ‡å‡†çš„Transformeræ¶æ„
- å……åˆ†è®­ç»ƒè‡³æ”¶æ•›
- ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹

#### æ­¥éª¤3: å­¦ç”Ÿæ¨¡å‹è®¾è®¡
- ç¡®å®šå‹ç¼©æ¯”ä¾‹
- è®¾è®¡ç½‘ç»œæ¶æ„
- åˆå§‹åŒ–å‚æ•°

#### æ­¥éª¤4: è’¸é¦è®­ç»ƒ
- è®¾ç½®æ¸©åº¦å‚æ•°T
- é…ç½®æŸå¤±æƒé‡Î±å’ŒÎ²
- æ‰§è¡Œè”åˆè®­ç»ƒ

#### æ­¥éª¤5: æ¨¡å‹ä¼˜åŒ–
- è¶…å‚æ•°è°ƒä¼˜
- å­¦ä¹ ç‡è°ƒåº¦
- æ­£åˆ™åŒ–æŠ€æœ¯

### 4.3 å…³é”®è¶…å‚æ•°è®¾ç½®

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| æ¸©åº¦T | 3.0-6.0 | æ§åˆ¶è½¯æ ‡ç­¾çš„å¹³æ»‘ç¨‹åº¦ |
| Î± (è’¸é¦æƒé‡) | 0.7-0.9 | è’¸é¦æŸå¤±çš„é‡è¦æ€§ |
| Î² (ç¡¬ç›®æ ‡æƒé‡) | 0.1-0.3 | çœŸå®æ ‡ç­¾çš„é‡è¦æ€§ |
| å­¦ä¹ ç‡ | 1e-4 to 5e-4 | é€šå¸¸æ¯”æ•™å¸ˆæ¨¡å‹è®­ç»ƒæ—¶æ›´å° |
| æ‰¹æ¬¡å¤§å° | 32-128 | æ ¹æ®GPUå†…å­˜è°ƒæ•´ |

---

## 5. ä»£ç å®ç°

### 5.1 è’¸é¦æŸå¤±å‡½æ•°å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DistillationLoss(nn.Module):
    """
    çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°
    ç»“åˆè½¯ç›®æ ‡æŸå¤±å’Œç¡¬ç›®æ ‡æŸå¤±
    """
    def __init__(self, temperature=4.0, alpha=0.8, beta=0.2):
        super(DistillationLoss, self).__init__()
        self.temperature = temperature
        self.alpha = alpha  # è’¸é¦æŸå¤±æƒé‡
        self.beta = beta    # ç¡¬ç›®æ ‡æŸå¤±æƒé‡
        self.kl_div = nn.KLDivLoss(reduction="batchmean")
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥padding
        
    def forward(self, student_logits, teacher_logits, targets):
        """
        è®¡ç®—è’¸é¦æŸå¤±
        
        Args:
            student_logits: å­¦ç”Ÿæ¨¡å‹è¾“å‡º [batch_size, seq_len, vocab_size]
            teacher_logits: æ•™å¸ˆæ¨¡å‹è¾“å‡º [batch_size, seq_len, vocab_size]
            targets: çœŸå®æ ‡ç­¾ [batch_size, seq_len]
        
        Returns:
            total_loss: æ€»æŸå¤±
            distill_loss: è’¸é¦æŸå¤±
            hard_loss: ç¡¬ç›®æ ‡æŸå¤±
        """
        # é‡å¡‘å¼ é‡ç”¨äºæŸå¤±è®¡ç®—
        batch_size, seq_len, vocab_size = student_logits.shape
        student_flat = student_logits.view(-1, vocab_size)
        teacher_flat = teacher_logits.view(-1, vocab_size)
        targets_flat = targets.view(-1)
        
        # åˆ›å»ºæ©ç ï¼Œå¿½ç•¥paddingä½ç½®
        mask = (targets_flat != 0)
        
        if mask.sum() == 0:
            return torch.tensor(0.0, device=student_logits.device)
        
        # åº”ç”¨æ©ç 
        student_masked = student_flat[mask]
        teacher_masked = teacher_flat[mask]
        targets_masked = targets_flat[mask]
        
        # è®¡ç®—è½¯ç›®æ ‡ (æ¸©åº¦ç¼©æ”¾)
        student_soft = F.log_softmax(student_masked / self.temperature, dim=-1)
        teacher_soft = F.softmax(teacher_masked / self.temperature, dim=-1)
        
        # è’¸é¦æŸå¤± (KLæ•£åº¦)
        distill_loss = self.kl_div(student_soft, teacher_soft) * (self.temperature ** 2)
        
        # ç¡¬ç›®æ ‡æŸå¤± (äº¤å‰ç†µ)
        hard_loss = self.ce_loss(student_masked, targets_masked)
        
        # æ€»æŸå¤±
        total_loss = self.alpha * distill_loss + self.beta * hard_loss
        
        return total_loss, distill_loss, hard_loss
```

### 5.2 å­¦ç”Ÿæ¨¡å‹æ¶æ„è®¾è®¡

```python
class CompactTransformer(nn.Module):
    """
    å‹ç¼©ç‰ˆTransformeræ¨¡å‹ (å­¦ç”Ÿæ¨¡å‹)
    """
    def __init__(self, 
                 vocab_size,
                 d_model=256,           # éšè—ç»´åº¦ (æ•™å¸ˆæ¨¡å‹çš„50%)
                 nhead=4,               # æ³¨æ„åŠ›å¤´æ•° (æ•™å¸ˆæ¨¡å‹çš„50%)
                 num_layers=6,          # å±‚æ•° (æ•™å¸ˆæ¨¡å‹çš„50%)
                 dim_feedforward=1024,  # å‰é¦ˆç½‘ç»œç»´åº¦
                 max_seq_length=512,
                 dropout=0.1):
        super(CompactTransformer, self).__init__()
        
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_seq_length)
        
        # Transformerç¼–ç å™¨å±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.output_projection = nn.Linear(d_model, vocab_size)
        
        # å‚æ•°åˆå§‹åŒ–
        self._init_parameters()
    
    def _init_parameters(self):
        """å‚æ•°åˆå§‹åŒ–"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        """å‰å‘ä¼ æ’­"""
        # åµŒå…¥å’Œä½ç½®ç¼–ç 
        src_emb = self.pos_encoding(self.embedding(src) * math.sqrt(self.d_model))
        tgt_emb = self.pos_encoding(self.embedding(tgt) * math.sqrt(self.d_model))
        
        # Transformerç¼–ç 
        memory = self.transformer(src_emb, src_mask)
        output = self.transformer(tgt_emb, tgt_mask, memory)
        
        # è¾“å‡ºæŠ•å½±
        logits = self.output_projection(output)
        
        return logits

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:x.size(0), :]
```

### 5.3 å¤šå±‚ç‰¹å¾è’¸é¦å®ç°

```python
class MultiLayerDistillationLoss(nn.Module):
    """
    å¤šå±‚ç‰¹å¾è’¸é¦æŸå¤±å‡½æ•°
    ä¸ä»…åœ¨è¾“å‡ºå±‚è¿›è¡Œè’¸é¦ï¼Œè¿˜åœ¨ä¸­é—´å±‚è¿›è¡Œç‰¹å¾å¯¹é½
    """
    def __init__(self, temperature=4.0, alpha=0.8, beta=0.2, gamma=0.1):
        super(MultiLayerDistillationLoss, self).__init__()
        self.temperature = temperature
        self.alpha = alpha      # è¾“å‡ºè’¸é¦æƒé‡
        self.beta = beta        # ç¡¬ç›®æ ‡æƒé‡
        self.gamma = gamma      # ç‰¹å¾è’¸é¦æƒé‡

        self.kl_div = nn.KLDivLoss(reduction="batchmean")
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=0)
        self.mse_loss = nn.MSELoss()

        # ç‰¹å¾å¯¹é½å±‚ (å°†å­¦ç”Ÿæ¨¡å‹ç‰¹å¾æ˜ å°„åˆ°æ•™å¸ˆæ¨¡å‹ç»´åº¦)
        self.feature_adapters = nn.ModuleList()

    def add_feature_adapter(self, student_dim, teacher_dim):
        """æ·»åŠ ç‰¹å¾é€‚é…å™¨"""
        adapter = nn.Linear(student_dim, teacher_dim)
        self.feature_adapters.append(adapter)

    def forward(self, student_outputs, teacher_outputs, targets):
        """
        è®¡ç®—å¤šå±‚è’¸é¦æŸå¤±

        Args:
            student_outputs: dictåŒ…å«logitså’Œä¸­é—´ç‰¹å¾
            teacher_outputs: dictåŒ…å«logitså’Œä¸­é—´ç‰¹å¾
            targets: çœŸå®æ ‡ç­¾
        """
        student_logits = student_outputs['logits']
        teacher_logits = teacher_outputs['logits']

        # è¾“å‡ºå±‚è’¸é¦æŸå¤±
        output_loss = self._compute_output_distillation(
            student_logits, teacher_logits, targets
        )

        # ç‰¹å¾å±‚è’¸é¦æŸå¤±
        feature_loss = self._compute_feature_distillation(
            student_outputs['features'], teacher_outputs['features']
        )

        # æ€»æŸå¤±
        total_loss = output_loss + self.gamma * feature_loss

        return total_loss, output_loss, feature_loss

    def _compute_output_distillation(self, student_logits, teacher_logits, targets):
        """è®¡ç®—è¾“å‡ºå±‚è’¸é¦æŸå¤±"""
        # é‡å¡‘å’Œæ©ç å¤„ç†
        batch_size, seq_len, vocab_size = student_logits.shape
        student_flat = student_logits.view(-1, vocab_size)
        teacher_flat = teacher_logits.view(-1, vocab_size)
        targets_flat = targets.view(-1)

        mask = (targets_flat != 0)
        if mask.sum() == 0:
            return torch.tensor(0.0, device=student_logits.device)

        student_masked = student_flat[mask]
        teacher_masked = teacher_flat[mask]
        targets_masked = targets_flat[mask]

        # è½¯ç›®æ ‡è’¸é¦
        student_soft = F.log_softmax(student_masked / self.temperature, dim=-1)
        teacher_soft = F.softmax(teacher_masked / self.temperature, dim=-1)
        distill_loss = self.kl_div(student_soft, teacher_soft) * (self.temperature ** 2)

        # ç¡¬ç›®æ ‡æŸå¤±
        hard_loss = self.ce_loss(student_masked, targets_masked)

        return self.alpha * distill_loss + self.beta * hard_loss

    def _compute_feature_distillation(self, student_features, teacher_features):
        """è®¡ç®—ç‰¹å¾å±‚è’¸é¦æŸå¤±"""
        feature_loss = 0

        for i, (s_feat, t_feat) in enumerate(zip(student_features, teacher_features)):
            # ç‰¹å¾ç»´åº¦å¯¹é½
            if i < len(self.feature_adapters):
                s_feat_aligned = self.feature_adapters[i](s_feat)
            else:
                s_feat_aligned = s_feat

            # MSEæŸå¤±
            feature_loss += self.mse_loss(s_feat_aligned, t_feat.detach())

        return feature_loss / len(student_features)
```

### 5.4 æ³¨æ„åŠ›è’¸é¦å®ç°

```python
class AttentionDistillationLoss(nn.Module):
    """
    æ³¨æ„åŠ›è’¸é¦æŸå¤±å‡½æ•°
    è’¸é¦æ•™å¸ˆæ¨¡å‹çš„æ³¨æ„åŠ›æ¨¡å¼
    """
    def __init__(self, temperature=4.0, alpha=0.8, beta=0.2, attention_weight=0.1):
        super(AttentionDistillationLoss, self).__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.beta = beta
        self.attention_weight = attention_weight

        self.kl_div = nn.KLDivLoss(reduction="batchmean")
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=0)
        self.mse_loss = nn.MSELoss()

    def forward(self, student_outputs, teacher_outputs, targets):
        """
        è®¡ç®—æ³¨æ„åŠ›è’¸é¦æŸå¤±
        """
        # åŸºç¡€è’¸é¦æŸå¤±
        base_loss = self._compute_base_distillation(
            student_outputs['logits'], teacher_outputs['logits'], targets
        )

        # æ³¨æ„åŠ›è’¸é¦æŸå¤±
        attention_loss = self._compute_attention_distillation(
            student_outputs['attentions'], teacher_outputs['attentions']
        )

        total_loss = base_loss + self.attention_weight * attention_loss

        return total_loss, base_loss, attention_loss

    def _compute_attention_distillation(self, student_attentions, teacher_attentions):
        """è®¡ç®—æ³¨æ„åŠ›è’¸é¦æŸå¤±"""
        attention_loss = 0
        num_layers = min(len(student_attentions), len(teacher_attentions))

        for layer_idx in range(num_layers):
            s_attn = student_attentions[layer_idx]  # [batch, heads, seq, seq]
            t_attn = teacher_attentions[layer_idx]

            # å¦‚æœæ³¨æ„åŠ›å¤´æ•°ä¸åŒï¼Œéœ€è¦è¿›è¡Œé€‚é…
            if s_attn.size(1) != t_attn.size(1):
                # ç®€å•ç­–ç•¥ï¼šé‡å¤å­¦ç”Ÿæ³¨æ„åŠ›å¤´æˆ–å¹³å‡æ•™å¸ˆæ³¨æ„åŠ›å¤´
                if s_attn.size(1) < t_attn.size(1):
                    # å¹³å‡æ•™å¸ˆæ³¨æ„åŠ›å¤´
                    t_attn = self._average_attention_heads(t_attn, s_attn.size(1))
                else:
                    # é‡å¤å­¦ç”Ÿæ³¨æ„åŠ›å¤´
                    s_attn = self._repeat_attention_heads(s_attn, t_attn.size(1))

            # è®¡ç®—æ³¨æ„åŠ›MSEæŸå¤±
            layer_loss = self.mse_loss(s_attn, t_attn.detach())
            attention_loss += layer_loss

        return attention_loss / num_layers

    def _average_attention_heads(self, attention, target_heads):
        """å¹³å‡æ³¨æ„åŠ›å¤´"""
        batch_size, num_heads, seq_len, _ = attention.shape
        heads_per_group = num_heads // target_heads

        # é‡å¡‘å¹¶å¹³å‡
        attention = attention.view(batch_size, target_heads, heads_per_group, seq_len, seq_len)
        return attention.mean(dim=2)

    def _repeat_attention_heads(self, attention, target_heads):
        """é‡å¤æ³¨æ„åŠ›å¤´"""
        batch_size, num_heads, seq_len, _ = attention.shape
        repeat_factor = target_heads // num_heads

        return attention.repeat(1, repeat_factor, 1, 1)
```

### 5.5 è’¸é¦è®­ç»ƒä¸»å¾ªç¯

```python
def distillation_training(teacher_model, student_model, train_loader, 
                         num_epochs=50, device='cuda'):
    """
    çŸ¥è¯†è’¸é¦è®­ç»ƒä¸»å‡½æ•°
    """
    # è®¾ç½®æ¨¡å‹çŠ¶æ€
    teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹è®¾ä¸ºè¯„ä¼°æ¨¡å¼
    student_model.train() # å­¦ç”Ÿæ¨¡å‹è®¾ä¸ºè®­ç»ƒæ¨¡å¼
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.AdamW(student_model.parameters(), 
                                 lr=1e-4, weight_decay=0.01)
    criterion = DistillationLoss(temperature=4.0, alpha=0.8, beta=0.2)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        total_loss = 0
        total_distill_loss = 0
        total_hard_loss = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
        
        for batch_idx, (src, tgt, targets) in enumerate(progress_bar):
            src, tgt, targets = src.to(device), tgt.to(device), targets.to(device)
            
            # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ (æ— æ¢¯åº¦)
            with torch.no_grad():
                teacher_logits = teacher_model(src, tgt)
            
            # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
            student_logits = student_model(src, tgt)
            
            # è®¡ç®—æŸå¤±
            loss, distill_loss, hard_loss = criterion(
                student_logits, teacher_logits, targets
            )
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # ç´¯ç§¯æŸå¤±
            total_loss += loss.item()
            total_distill_loss += distill_loss.item()
            total_hard_loss += hard_loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Distill': f'{distill_loss.item():.4f}',
                'Hard': f'{hard_loss.item():.4f}'
            })
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # æ‰“å°epochç»Ÿè®¡
        avg_loss = total_loss / len(train_loader)
        avg_distill = total_distill_loss / len(train_loader)
        avg_hard = total_hard_loss / len(train_loader)
        
        print(f"Epoch {epoch+1} - Loss: {avg_loss:.4f}, "
              f"Distill: {avg_distill:.4f}, Hard: {avg_hard:.4f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 10 == 0:
            torch.save(student_model.state_dict(), 
                      f'student_model_epoch_{epoch+1}.pth')
    
    return student_model

### 5.6 æ¸è¿›å¼è’¸é¦å®ç°

```python
class ProgressiveDistillation:
    """
    æ¸è¿›å¼çŸ¥è¯†è’¸é¦
    é€šè¿‡å¤šä¸ªä¸­é—´æ¨¡å‹é€æ­¥å‹ç¼©ï¼Œé¿å…æ•™å¸ˆ-å­¦ç”Ÿå·®è·è¿‡å¤§
    """
    def __init__(self, teacher_model, target_compression_ratio=0.25):
        self.teacher_model = teacher_model
        self.target_ratio = target_compression_ratio
        self.intermediate_models = []
        self.compression_stages = self._plan_compression_stages()

    def _plan_compression_stages(self):
        """è§„åˆ’å‹ç¼©é˜¶æ®µ"""
        # ä»1.0é€æ­¥å‹ç¼©åˆ°ç›®æ ‡æ¯”ä¾‹
        stages = []
        current_ratio = 1.0

        while current_ratio > self.target_ratio:
            next_ratio = max(current_ratio * 0.7, self.target_ratio)
            stages.append(next_ratio)
            current_ratio = next_ratio

        return stages

    def create_intermediate_model(self, compression_ratio):
        """åˆ›å»ºä¸­é—´å‹ç¼©æ¨¡å‹"""
        teacher_config = self._extract_model_config(self.teacher_model)

        # è®¡ç®—å‹ç¼©åçš„é…ç½®
        compressed_config = {
            'd_model': int(teacher_config['d_model'] * math.sqrt(compression_ratio)),
            'num_layers': max(1, int(teacher_config['num_layers'] * compression_ratio)),
            'nhead': max(1, int(teacher_config['nhead'] * math.sqrt(compression_ratio))),
            'vocab_size': teacher_config['vocab_size'],
            'max_seq_length': teacher_config['max_seq_length']
        }

        # ç¡®ä¿ç»´åº¦èƒ½è¢«æ³¨æ„åŠ›å¤´æ•°æ•´é™¤
        compressed_config['d_model'] = (compressed_config['d_model'] //
                                      compressed_config['nhead']) * compressed_config['nhead']

        return CompactTransformer(**compressed_config)

    def progressive_train(self, train_loader, device='cuda'):
        """æ‰§è¡Œæ¸è¿›å¼è®­ç»ƒ"""
        current_teacher = self.teacher_model

        for stage_idx, compression_ratio in enumerate(self.compression_stages):
            print(f"\n=== æ¸è¿›å¼è’¸é¦é˜¶æ®µ {stage_idx + 1}: å‹ç¼©æ¯” {compression_ratio:.2f} ===")

            # åˆ›å»ºå½“å‰é˜¶æ®µçš„å­¦ç”Ÿæ¨¡å‹
            student_model = self.create_intermediate_model(compression_ratio)
            student_model.to(device)

            # æ‰§è¡Œè’¸é¦è®­ç»ƒ
            student_model = distillation_training(
                current_teacher, student_model, train_loader,
                num_epochs=20, device=device
            )

            # ä¿å­˜ä¸­é—´æ¨¡å‹
            torch.save(student_model.state_dict(),
                      f'progressive_model_stage_{stage_idx + 1}.pth')

            # å½“å‰å­¦ç”Ÿæ¨¡å‹æˆä¸ºä¸‹ä¸€é˜¶æ®µçš„æ•™å¸ˆæ¨¡å‹
            current_teacher = student_model
            self.intermediate_models.append(student_model)

        return self.intermediate_models[-1]  # è¿”å›æœ€ç»ˆçš„å­¦ç”Ÿæ¨¡å‹

### 5.7 è‡ªé€‚åº”æ¸©åº¦è°ƒåº¦

```python
class AdaptiveTemperatureScheduler:
    """
    è‡ªé€‚åº”æ¸©åº¦è°ƒåº¦å™¨
    æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´æ¸©åº¦å‚æ•°
    """
    def __init__(self, initial_temp=6.0, final_temp=2.0,
                 schedule_type='exponential', total_steps=10000):
        self.initial_temp = initial_temp
        self.final_temp = final_temp
        self.schedule_type = schedule_type
        self.total_steps = total_steps
        self.current_step = 0

    def get_temperature(self):
        """è·å–å½“å‰æ¸©åº¦"""
        progress = min(self.current_step / self.total_steps, 1.0)

        if self.schedule_type == 'exponential':
            # æŒ‡æ•°è¡°å‡
            temp = self.initial_temp * (self.final_temp / self.initial_temp) ** progress
        elif self.schedule_type == 'linear':
            # çº¿æ€§è¡°å‡
            temp = self.initial_temp - (self.initial_temp - self.final_temp) * progress
        elif self.schedule_type == 'cosine':
            # ä½™å¼¦è¡°å‡
            temp = self.final_temp + (self.initial_temp - self.final_temp) * \
                   (1 + math.cos(math.pi * progress)) / 2
        else:
            temp = self.initial_temp

        return temp

    def step(self):
        """æ›´æ–°æ­¥æ•°"""
        self.current_step += 1

class DynamicDistillationLoss(nn.Module):
    """
    åŠ¨æ€è’¸é¦æŸå¤±å‡½æ•°
    é›†æˆè‡ªé€‚åº”æ¸©åº¦è°ƒåº¦
    """
    def __init__(self, temp_scheduler, alpha=0.8, beta=0.2):
        super(DynamicDistillationLoss, self).__init__()
        self.temp_scheduler = temp_scheduler
        self.alpha = alpha
        self.beta = beta
        self.kl_div = nn.KLDivLoss(reduction="batchmean")
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=0)

    def forward(self, student_logits, teacher_logits, targets):
        """è®¡ç®—åŠ¨æ€è’¸é¦æŸå¤±"""
        # è·å–å½“å‰æ¸©åº¦
        current_temp = self.temp_scheduler.get_temperature()

        # è®¡ç®—æŸå¤± (ä½¿ç”¨å½“å‰æ¸©åº¦)
        batch_size, seq_len, vocab_size = student_logits.shape
        student_flat = student_logits.view(-1, vocab_size)
        teacher_flat = teacher_logits.view(-1, vocab_size)
        targets_flat = targets.view(-1)

        mask = (targets_flat != 0)
        if mask.sum() == 0:
            return torch.tensor(0.0, device=student_logits.device), current_temp

        student_masked = student_flat[mask]
        teacher_masked = teacher_flat[mask]
        targets_masked = targets_flat[mask]

        # ä½¿ç”¨åŠ¨æ€æ¸©åº¦çš„è½¯ç›®æ ‡
        student_soft = F.log_softmax(student_masked / current_temp, dim=-1)
        teacher_soft = F.softmax(teacher_masked / current_temp, dim=-1)
        distill_loss = self.kl_div(student_soft, teacher_soft) * (current_temp ** 2)

        # ç¡¬ç›®æ ‡æŸå¤±
        hard_loss = self.ce_loss(student_masked, targets_masked)

        # æ€»æŸå¤±
        total_loss = self.alpha * distill_loss + self.beta * hard_loss

        # æ›´æ–°æ¸©åº¦è°ƒåº¦å™¨
        self.temp_scheduler.step()

        return total_loss, current_temp

### 5.8 çŸ¥è¯†è’¸é¦è¯„ä¼°å·¥å…·

```python
class DistillationEvaluator:
    """
    çŸ¥è¯†è’¸é¦è¯„ä¼°å·¥å…·
    æä¾›å…¨é¢çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°
    """
    def __init__(self, teacher_model, student_model, tokenizer, device='cuda'):
        self.teacher_model = teacher_model
        self.student_model = student_model
        self.tokenizer = tokenizer
        self.device = device

    def evaluate_compression_ratio(self):
        """è¯„ä¼°æ¨¡å‹å‹ç¼©æ¯”"""
        teacher_params = sum(p.numel() for p in self.teacher_model.parameters())
        student_params = sum(p.numel() for p in self.student_model.parameters())

        compression_ratio = teacher_params / student_params
        size_reduction = (1 - student_params / teacher_params) * 100

        return {
            'teacher_params': teacher_params,
            'student_params': student_params,
            'compression_ratio': compression_ratio,
            'size_reduction_percent': size_reduction
        }

    def evaluate_inference_speed(self, test_sentences, num_runs=100):
        """è¯„ä¼°æ¨ç†é€Ÿåº¦"""
        import time

        # é¢„çƒ­
        for _ in range(10):
            self._translate_sentence(self.teacher_model, test_sentences[0])
            self._translate_sentence(self.student_model, test_sentences[0])

        # æµ‹è¯•æ•™å¸ˆæ¨¡å‹é€Ÿåº¦
        teacher_times = []
        for sentence in test_sentences:
            start_time = time.time()
            for _ in range(num_runs):
                self._translate_sentence(self.teacher_model, sentence)
            end_time = time.time()
            teacher_times.append((end_time - start_time) / num_runs)

        # æµ‹è¯•å­¦ç”Ÿæ¨¡å‹é€Ÿåº¦
        student_times = []
        for sentence in test_sentences:
            start_time = time.time()
            for _ in range(num_runs):
                self._translate_sentence(self.student_model, sentence)
            end_time = time.time()
            student_times.append((end_time - start_time) / num_runs)

        avg_teacher_time = sum(teacher_times) / len(teacher_times)
        avg_student_time = sum(student_times) / len(student_times)
        speedup = avg_teacher_time / avg_student_time

        return {
            'teacher_avg_time': avg_teacher_time,
            'student_avg_time': avg_student_time,
            'speedup': speedup
        }

    def evaluate_translation_quality(self, test_pairs):
        """è¯„ä¼°ç¿»è¯‘è´¨é‡"""
        from nltk.translate.bleu_score import sentence_bleu

        teacher_bleu_scores = []
        student_bleu_scores = []

        for src_sentence, ref_translation in test_pairs:
            # æ•™å¸ˆæ¨¡å‹ç¿»è¯‘
            teacher_translation = self._translate_sentence(self.teacher_model, src_sentence)
            teacher_bleu = sentence_bleu([ref_translation.split()],
                                       teacher_translation.split())
            teacher_bleu_scores.append(teacher_bleu)

            # å­¦ç”Ÿæ¨¡å‹ç¿»è¯‘
            student_translation = self._translate_sentence(self.student_model, src_sentence)
            student_bleu = sentence_bleu([ref_translation.split()],
                                       student_translation.split())
            student_bleu_scores.append(student_bleu)

        return {
            'teacher_avg_bleu': sum(teacher_bleu_scores) / len(teacher_bleu_scores),
            'student_avg_bleu': sum(student_bleu_scores) / len(student_bleu_scores),
            'quality_retention': (sum(student_bleu_scores) / sum(teacher_bleu_scores)) * 100
        }

    def _translate_sentence(self, model, sentence):
        """ç¿»è¯‘å•ä¸ªå¥å­"""
        model.eval()
        with torch.no_grad():
            # ç®€åŒ–çš„ç¿»è¯‘é€»è¾‘ (å®é™…å®ç°éœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹è°ƒæ•´)
            tokens = self.tokenizer.encode(sentence)
            input_ids = torch.tensor([tokens]).to(self.device)

            # ç”Ÿæˆç¿»è¯‘ (è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ¨¡å‹æ¥å£è°ƒæ•´)
            output = model.generate(input_ids, max_length=50)
            translation = self.tokenizer.decode(output[0])

            return translation

    def comprehensive_evaluation(self, test_sentences, test_pairs):
        """ç»¼åˆè¯„ä¼°"""
        print("=== çŸ¥è¯†è’¸é¦æ¨¡å‹ç»¼åˆè¯„ä¼° ===\n")

        # å‹ç¼©æ¯”è¯„ä¼°
        compression_results = self.evaluate_compression_ratio()
        print("ğŸ“Š æ¨¡å‹å‹ç¼©æ•ˆæœ:")
        print(f"  æ•™å¸ˆæ¨¡å‹å‚æ•°: {compression_results['teacher_params']:,}")
        print(f"  å­¦ç”Ÿæ¨¡å‹å‚æ•°: {compression_results['student_params']:,}")
        print(f"  å‹ç¼©æ¯”: {compression_results['compression_ratio']:.2f}x")
        print(f"  å‚æ•°å‡å°‘: {compression_results['size_reduction_percent']:.1f}%\n")

        # æ¨ç†é€Ÿåº¦è¯„ä¼°
        speed_results = self.evaluate_inference_speed(test_sentences)
        print("âš¡ æ¨ç†é€Ÿåº¦å¯¹æ¯”:")
        print(f"  æ•™å¸ˆæ¨¡å‹å¹³å‡æ—¶é—´: {speed_results['teacher_avg_time']:.4f}s")
        print(f"  å­¦ç”Ÿæ¨¡å‹å¹³å‡æ—¶é—´: {speed_results['student_avg_time']:.4f}s")
        print(f"  é€Ÿåº¦æå‡: {speed_results['speedup']:.2f}x\n")

        # ç¿»è¯‘è´¨é‡è¯„ä¼°
        quality_results = self.evaluate_translation_quality(test_pairs)
        print("ğŸ¯ ç¿»è¯‘è´¨é‡å¯¹æ¯”:")
        print(f"  æ•™å¸ˆæ¨¡å‹å¹³å‡BLEU: {quality_results['teacher_avg_bleu']:.4f}")
        print(f"  å­¦ç”Ÿæ¨¡å‹å¹³å‡BLEU: {quality_results['student_avg_bleu']:.4f}")
        print(f"  è´¨é‡ä¿æŒç‡: {quality_results['quality_retention']:.1f}%\n")

        return {
            'compression': compression_results,
            'speed': speed_results,
            'quality': quality_results
        }
```

---

## 6. ä¼˜åŒ–ç­–ç•¥

### 6.1 æ¸è¿›å¼è’¸é¦

ä¼ ç»Ÿè’¸é¦å¯èƒ½å­˜åœ¨æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹å·®è·è¿‡å¤§çš„é—®é¢˜ã€‚æ¸è¿›å¼è’¸é¦é€šè¿‡å¤šä¸ªä¸­é—´æ¨¡å‹é€æ­¥å‹ç¼©ï¼š

```
æ•™å¸ˆæ¨¡å‹ â†’ ä¸­é—´æ¨¡å‹1 â†’ ä¸­é—´æ¨¡å‹2 â†’ å­¦ç”Ÿæ¨¡å‹
  (100%)     (75%)        (50%)       (25%)
```

### 6.2 å¤šå±‚è’¸é¦

ä¸ä»…åœ¨è¾“å‡ºå±‚è¿›è¡Œè’¸é¦ï¼Œè¿˜åœ¨ä¸­é—´å±‚è¿›è¡Œç‰¹å¾è’¸é¦ï¼š

$$L_{feature} = \sum_{l=1}^{L} \lambda_l \cdot MSE(f_l^{student}, f_l^{teacher})$$

å…¶ä¸­ $f_l$ è¡¨ç¤ºç¬¬lå±‚çš„ç‰¹å¾è¡¨ç¤ºã€‚

### 6.3 æ³¨æ„åŠ›è’¸é¦

è’¸é¦æ•™å¸ˆæ¨¡å‹çš„æ³¨æ„åŠ›æ¨¡å¼ï¼š

$$L_{attention} = \sum_{h=1}^{H} MSE(A_h^{student}, A_h^{teacher})$$

å…¶ä¸­ $A_h$ è¡¨ç¤ºç¬¬hä¸ªæ³¨æ„åŠ›å¤´çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µã€‚

### 6.4 è‡ªé€‚åº”æ¸©åº¦è°ƒåº¦

åŠ¨æ€è°ƒæ•´æ¸©åº¦å‚æ•°çš„æ•°å­¦å…¬å¼ï¼š

**æŒ‡æ•°è¡°å‡è°ƒåº¦**ï¼š
$$T(t) = T_0 \cdot \left(\frac{T_{final}}{T_0}\right)^{t/T_{total}}$$

**çº¿æ€§è¡°å‡è°ƒåº¦**ï¼š
$$T(t) = T_0 - (T_0 - T_{final}) \cdot \frac{t}{T_{total}}$$

**ä½™å¼¦è¡°å‡è°ƒåº¦**ï¼š
$$T(t) = T_{final} + (T_0 - T_{final}) \cdot \frac{1 + \cos(\pi \cdot t/T_{total})}{2}$$

å…¶ä¸­ï¼š
- $T_0$ æ˜¯åˆå§‹æ¸©åº¦ (é€šå¸¸ä¸º4.0-6.0)
- $T_{final}$ æ˜¯æœ€ç»ˆæ¸©åº¦ (é€šå¸¸ä¸º2.0-3.0)
- $t$ æ˜¯å½“å‰è®­ç»ƒæ­¥æ•°
- $T_{total}$ æ˜¯æ€»è®­ç»ƒæ­¥æ•°

### 6.5 åœ¨çº¿è’¸é¦ç­–ç•¥

åœ¨çº¿è’¸é¦å…è®¸æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹åŒæ—¶è®­ç»ƒï¼Œé¿å…äº†é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹çš„éœ€è¦ï¼š

**äº’ç›¸å­¦ä¹ æŸå¤±**ï¼š
$$L_{mutual} = \frac{1}{2}[KL(P_1||P_2) + KL(P_2||P_1)]$$

**æ·±åº¦äº’ç›¸å­¦ä¹ **ï¼š
$$L_{DML} = L_{CE}(P_1, y) + L_{CE}(P_2, y) + \lambda \cdot L_{mutual}$$

### 6.6 å¤šæ•™å¸ˆè’¸é¦

é›†æˆå¤šä¸ªæ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ï¼š

**åŠ æƒå¹³å‡ç­–ç•¥**ï¼š
$$P_{ensemble}(x) = \sum_{i=1}^{N} w_i \cdot P_i(x)$$

**æ³¨æ„åŠ›åŠ æƒç­–ç•¥**ï¼š
$$w_i = \frac{\exp(\alpha_i)}{\sum_{j=1}^{N} \exp(\alpha_j)}$$

å…¶ä¸­ $\alpha_i$ æ˜¯å¯å­¦ä¹ çš„æ³¨æ„åŠ›æƒé‡ã€‚

### 6.7 è·¨è¯­è¨€è’¸é¦

åˆ©ç”¨é«˜èµ„æºè¯­è¨€å¸®åŠ©ä½èµ„æºè¯­è¨€çš„ç¿»è¯‘ï¼š

**è¯­è¨€æ— å…³è¡¨ç¤ºå­¦ä¹ **ï¼š
$$L_{cross} = L_{distill}(P_{student}^{low}, P_{teacher}^{high}) + L_{align}(H_{low}, H_{high})$$

å…¶ä¸­ $L_{align}$ æ˜¯ç‰¹å¾å¯¹é½æŸå¤±ã€‚

---

## 7. å®éªŒç»“æœåˆ†æ

### 7.1 æ€§èƒ½æŒ‡æ ‡

**æ¨¡å‹å‹ç¼©æ•ˆæœ**:
- å‚æ•°é‡å‡å°‘: 60-80%
- æ¨ç†é€Ÿåº¦æå‡: 2-4å€
- å†…å­˜å ç”¨å‡å°‘: 50-70%

**ç¿»è¯‘è´¨é‡ä¿æŒ**:
- BLEUåˆ†æ•°ä¿æŒ: 85-95%
- è¯­ä¹‰ç›¸ä¼¼åº¦: 90-98%
- æµç•…åº¦è¯„åˆ†: 88-96%

### 7.2 æ¶ˆèå®éªŒ

| é…ç½® | BLEU | å‚æ•°é‡ | æ¨ç†é€Ÿåº¦ |
|------|------|--------|----------|
| æ•™å¸ˆæ¨¡å‹ | 35.2 | 100M | 1.0x |
| æ— è’¸é¦å­¦ç”Ÿ | 28.4 | 25M | 3.2x |
| æ ‡å‡†è’¸é¦ | 32.1 | 25M | 3.2x |
| å¤šå±‚è’¸é¦ | 33.5 | 25M | 3.2x |
| æ³¨æ„åŠ›è’¸é¦ | 33.8 | 25M | 3.2x |

### 7.3 è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ

**æ¸©åº¦å‚æ•°Tçš„å½±å“**:

| æ¸©åº¦T | BLEUåˆ†æ•° | è®­ç»ƒç¨³å®šæ€§ | æ”¶æ•›é€Ÿåº¦ | æ¨èåœºæ™¯ |
|-------|----------|------------|----------|----------|
| T=1.0 | 28.4 | é«˜ | å¿« | åŸºçº¿å¯¹æ¯” |
| T=2.0 | 31.2 | é«˜ | ä¸­ç­‰ | ä¿å®ˆè®­ç»ƒ |
| T=4.0 | 33.8 | ä¸­ç­‰ | ä¸­ç­‰ | **æ¨èå€¼** |
| T=6.0 | 33.5 | ä¸­ç­‰ | æ…¢ | å¤§æ¨¡å‹å·®è· |
| T=10.0 | 31.9 | ä½ | å¾ˆæ…¢ | ä¸æ¨è |

**æƒé‡æ¯”ä¾‹Î±/Î²çš„å½±å“**:

| Î±/Î²æ¯”ä¾‹ | BLEUåˆ†æ•° | é€‚ç”¨åœºæ™¯ | è®­ç»ƒéš¾åº¦ |
|---------|----------|----------|----------|
| 0.9/0.1 | 33.2 | å¤§å·®è·æ¨¡å‹ | é«˜ |
| 0.8/0.2 | 33.8 | **é€šç”¨æ¨è** | ä¸­ç­‰ |
| 0.7/0.3 | 33.5 | å¹³è¡¡è®­ç»ƒ | ä¸­ç­‰ |
| 0.5/0.5 | 32.1 | å°å·®è·æ¨¡å‹ | ä½ |

**å­¦ä¹ ç‡è°ƒåº¦çš„å½±å“**:

```python
# ä¸åŒå­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥çš„æ•ˆæœå¯¹æ¯”
learning_rate_strategies = {
    'constant': {'final_bleu': 32.1, 'convergence_epoch': 45},
    'step_decay': {'final_bleu': 33.2, 'convergence_epoch': 38},
    'cosine_annealing': {'final_bleu': 33.8, 'convergence_epoch': 35},
    'warmup_cosine': {'final_bleu': 34.1, 'convergence_epoch': 32}
}
```

### 7.4 ä¸åŒè’¸é¦ç­–ç•¥å¯¹æ¯”

| è’¸é¦ç­–ç•¥ | BLEU | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | å®ç°å¤æ‚åº¦ |
|----------|------|--------|----------|------------|
| **åŸºç¡€è’¸é¦** | 32.1 | 25M | 1.0x | ä½ |
| **å¤šå±‚è’¸é¦** | 33.5 | 25M | 1.3x | ä¸­ç­‰ |
| **æ³¨æ„åŠ›è’¸é¦** | 33.8 | 25M | 1.5x | ä¸­ç­‰ |
| **æ¸è¿›å¼è’¸é¦** | 34.2 | 25M | 2.0x | é«˜ |
| **è‡ªé€‚åº”æ¸©åº¦** | 33.9 | 25M | 1.1x | ä½ |
| **ç»„åˆç­–ç•¥** | 34.6 | 25M | 2.2x | é«˜ |

### 7.5 ä¸åŒè¯­è¨€å¯¹çš„è’¸é¦æ•ˆæœ

| è¯­è¨€å¯¹ | æ•™å¸ˆBLEU | å­¦ç”ŸBLEU | è´¨é‡ä¿æŒç‡ | å‹ç¼©æ¯” |
|--------|----------|----------|------------|--------|
| Enâ†’De | 28.5 | 26.1 | 91.6% | 4.0x |
| Enâ†’Fr | 32.1 | 29.8 | 92.8% | 4.0x |
| Enâ†’Zh | 24.3 | 21.9 | 90.1% | 4.0x |
| Enâ†’Hu | 22.8 | 20.5 | 89.9% | 4.0x |
| Enâ†’Ja | 19.6 | 17.2 | 87.8% | 4.0x |

**è§‚å¯Ÿç»“æœ**:
- å½¢æ€ä¸°å¯Œçš„è¯­è¨€(å¦‚åŒˆç‰™åˆ©è¯­ã€æ—¥è¯­)è’¸é¦æ•ˆæœç›¸å¯¹è¾ƒå·®
- è¯­è¨€ç›¸ä¼¼åº¦è¶Šé«˜ï¼Œè’¸é¦æ•ˆæœè¶Šå¥½
- ä½èµ„æºè¯­è¨€å¯¹è’¸é¦æ›´åŠ æ•æ„Ÿ

### 7.6 è®¡ç®—æ•ˆç‡åˆ†æ

**å†…å­˜ä½¿ç”¨å¯¹æ¯”**:
```python
memory_usage = {
    'teacher_model': {
        'parameters': '169 MB',
        'activations': '2.1 GB',
        'total_training': '4.8 GB'
    },
    'student_model': {
        'parameters': '42 MB',
        'activations': '0.8 GB',
        'total_training': '2.1 GB'
    },
    'distillation_training': {
        'both_models': '211 MB',
        'activations': '2.9 GB',
        'total_training': '5.2 GB'
    }
}
```

**æ¨ç†å»¶è¿Ÿåˆ†æ**:
```python
latency_analysis = {
    'batch_size_1': {
        'teacher': '45ms',
        'student': '12ms',
        'speedup': '3.75x'
    },
    'batch_size_32': {
        'teacher': '180ms',
        'student': '58ms',
        'speedup': '3.10x'
    },
    'batch_size_128': {
        'teacher': '650ms',
        'student': '220ms',
        'speedup': '2.95x'
    }
}
```

### 7.7 é”™è¯¯åˆ†æ

**å¸¸è§è®­ç»ƒé—®é¢˜åŠè§£å†³æ–¹æ¡ˆ**:

1. **æ¢¯åº¦çˆ†ç‚¸**:
   - ç°è±¡: æŸå¤±çªç„¶å¢å¤§ï¼Œæ¨¡å‹å‘æ•£
   - è§£å†³: é™ä½å­¦ä¹ ç‡ï¼Œå¢å¼ºæ¢¯åº¦è£å‰ª

2. **æ¨¡å¼åå¡Œ**:
   - ç°è±¡: å­¦ç”Ÿæ¨¡å‹æ€»æ˜¯è¾“å‡ºç›¸åŒtoken
   - è§£å†³: é™ä½æ¸©åº¦å‚æ•°ï¼Œå¢åŠ ç¡¬ç›®æ ‡æƒé‡

3. **æ”¶æ•›ç¼“æ…¢**:
   - ç°è±¡: æŸå¤±ä¸‹é™å¾ˆæ…¢
   - è§£å†³: å¢åŠ å­¦ä¹ ç‡ï¼Œä½¿ç”¨å­¦ä¹ ç‡é¢„çƒ­

4. **è´¨é‡ä¸‹é™ä¸¥é‡**:
   - ç°è±¡: å­¦ç”Ÿæ¨¡å‹BLEUåˆ†æ•°è¿‡ä½
   - è§£å†³: å‡å°‘å‹ç¼©æ¯”ï¼Œä½¿ç”¨æ¸è¿›å¼è’¸é¦

**è´¨é‡ä¿æŒç­–ç•¥**:
```python
quality_preservation_techniques = {
    'architecture_design': {
        'min_compression_ratio': 2.0,  # ä¸è¦è¿‡åº¦å‹ç¼©
        'preserve_attention_heads': True,  # ä¿æŒè¶³å¤Ÿçš„æ³¨æ„åŠ›å¤´
        'layer_wise_compression': True  # åˆ†å±‚å‹ç¼©ç­–ç•¥
    },
    'training_strategy': {
        'progressive_distillation': True,
        'multi_layer_distillation': True,
        'attention_transfer': True,
        'curriculum_learning': True
    },
    'hyperparameter_tuning': {
        'temperature_scheduling': True,
        'adaptive_weights': True,
        'early_stopping': True
    }
}
```

---

## 8. æ€»ç»“ä¸å±•æœ›

### 8.1 æŠ€æœ¯æ€»ç»“

çŸ¥è¯†è’¸é¦ä¸ºTransformeræœºå™¨ç¿»è¯‘æ¨¡å‹æä¾›äº†ä¸€å¥—å®Œæ•´çš„å‹ç¼©ä¼˜åŒ–æ–¹æ¡ˆï¼š

#### 8.1.1 æ ¸å¿ƒæŠ€æœ¯ä¼˜åŠ¿

1. **ç†è®ºåŸºç¡€æ‰å®**:
   - åŸºäºä¿¡æ¯è®ºçš„è½¯æ ‡ç­¾çŸ¥è¯†ä¼ é€’
   - æ¸©åº¦ç¼©æ”¾æœºåˆ¶æœ‰æ•ˆæ§åˆ¶çŸ¥è¯†ä¼ é€’å¼ºåº¦
   - å¤šå±‚æ¬¡ç‰¹å¾å¯¹é½ç¡®ä¿çŸ¥è¯†å®Œæ•´æ€§

2. **å®ç°çµæ´»é«˜æ•ˆ**:
   - æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰è®­ç»ƒæµç¨‹
   - æ”¯æŒå¤šç§è’¸é¦ç­–ç•¥çš„ç»„åˆä½¿ç”¨
   - è‡ªé€‚åº”è¶…å‚æ•°è°ƒæ•´å‡å°‘äººå·¥è°ƒä¼˜

3. **æ•ˆæœæ˜¾è‘—å¯é **:
   - 2-4å€æ¨¡å‹å‹ç¼©ï¼Œè´¨é‡ä¿æŒ85-95%
   - æ¨ç†é€Ÿåº¦æå‡2-4å€ï¼Œå†…å­˜å ç”¨å‡å°‘50-70%
   - åœ¨å¤šç§è¯­è¨€å¯¹ä¸ŠéªŒè¯æœ‰æ•ˆæ€§

4. **é€‚ç”¨æ€§å¹¿æ³›**:
   - æ”¯æŒå„ç§Transformeræ¶æ„å˜ä½“
   - å¯æ‰©å±•åˆ°å…¶ä»–åºåˆ—åˆ°åºåˆ—ä»»åŠ¡
   - å…¼å®¹ä¸åŒè§„æ¨¡çš„æ¨¡å‹å‹ç¼©éœ€æ±‚

#### 8.1.2 å…³é”®æŠ€æœ¯åˆ›æ–°

**å¤šå±‚æ¬¡çŸ¥è¯†è’¸é¦æ¡†æ¶**:
```python
distillation_framework = {
    'output_level': 'softmaxæ¦‚ç‡åˆ†å¸ƒè’¸é¦',
    'feature_level': 'ä¸­é—´å±‚ç‰¹å¾å¯¹é½',
    'attention_level': 'æ³¨æ„åŠ›æ¨¡å¼ä¼ é€’',
    'structural_level': 'æ¶æ„çŸ¥è¯†è¿ç§»'
}
```

**è‡ªé€‚åº”è®­ç»ƒç­–ç•¥**:
- åŠ¨æ€æ¸©åº¦è°ƒåº¦ä¼˜åŒ–çŸ¥è¯†ä¼ é€’è¿‡ç¨‹
- æ¸è¿›å¼å‹ç¼©é¿å…æ€§èƒ½æ–­å´–å¼ä¸‹é™
- å¤šç›®æ ‡æŸå¤±å‡½æ•°å¹³è¡¡ä¸åŒå­¦ä¹ ç›®æ ‡

### 8.2 æœ€ä½³å®è·µæŒ‡å—

#### 8.2.1 æ¨¡å‹è®¾è®¡åŸåˆ™

1. **å‹ç¼©æ¯”é€‰æ‹©**:
   - ä¿å®ˆå‹ç¼©: 1.5-2.0x (è´¨é‡ä¼˜å…ˆ)
   - å¹³è¡¡å‹ç¼©: 2.0-3.0x (è´¨é‡æ•ˆç‡å¹³è¡¡)
   - æ¿€è¿›å‹ç¼©: 3.0-4.0x (æ•ˆç‡ä¼˜å…ˆ)

2. **æ¶æ„è®¾è®¡ç­–ç•¥**:
   ```python
   architecture_guidelines = {
       'depth_compression': 'ä¼˜å…ˆå‹ç¼©å±‚æ•°ï¼Œä¿æŒå®½åº¦',
       'width_compression': 'é€‚åº¦å‹ç¼©éšè—ç»´åº¦',
       'attention_preservation': 'ä¿æŒè¶³å¤Ÿçš„æ³¨æ„åŠ›å¤´æ•°',
       'bottleneck_avoidance': 'é¿å…è¿‡åº¦å‹ç¼©é€ æˆä¿¡æ¯ç“¶é¢ˆ'
   }
   ```

3. **è®­ç»ƒé…ç½®ä¼˜åŒ–**:
   - å­¦ä¹ ç‡: æ•™å¸ˆæ¨¡å‹çš„50-70%
   - æ‰¹æ¬¡å¤§å°: æ ¹æ®GPUå†…å­˜é€‚å½“è°ƒæ•´
   - è®­ç»ƒè½®æ•°: é€šå¸¸ä¸ºæ•™å¸ˆæ¨¡å‹çš„30-50%

#### 8.2.2 è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥

**æ¸©åº¦å‚æ•°Tä¼˜åŒ–**:
```python
temperature_optimization = {
    'initialization': 4.0,  # åˆå§‹æ¸©åº¦
    'scheduling': 'cosine_decay',  # è°ƒåº¦ç­–ç•¥
    'final_value': 2.0,  # æœ€ç»ˆæ¸©åº¦
    'adaptation': 'loss_based'  # è‡ªé€‚åº”è°ƒæ•´
}
```

**æŸå¤±æƒé‡ä¼˜åŒ–**:
```python
loss_weight_optimization = {
    'distillation_weight': 0.8,  # è’¸é¦æŸå¤±æƒé‡
    'hard_target_weight': 0.2,   # ç¡¬ç›®æ ‡æƒé‡
    'feature_weight': 0.1,       # ç‰¹å¾è’¸é¦æƒé‡
    'attention_weight': 0.05     # æ³¨æ„åŠ›è’¸é¦æƒé‡
}
```

### 8.3 äº§ä¸šåŒ–åº”ç”¨æŒ‡å—

#### 8.3.1 éƒ¨ç½²åœºæ™¯åˆ†æ

| åº”ç”¨åœºæ™¯ | æ¨èå‹ç¼©æ¯” | è´¨é‡è¦æ±‚ | å»¶è¿Ÿè¦æ±‚ | èµ„æºé™åˆ¶ |
|----------|------------|----------|----------|----------|
| **ç§»åŠ¨ç«¯APP** | 3-4x | ä¸­ç­‰ | <100ms | ä¸¥æ ¼ |
| **è¾¹ç¼˜è®¡ç®—** | 2-3x | é«˜ | <50ms | ä¸­ç­‰ |
| **äº‘ç«¯æœåŠ¡** | 1.5-2x | å¾ˆé«˜ | <20ms | å®½æ¾ |
| **ç¦»çº¿ç¿»è¯‘** | 4-5x | ä¸­ç­‰ | å®½æ¾ | å¾ˆä¸¥æ ¼ |

#### 8.3.2 è´¨é‡ç›‘æ§ä½“ç³»

```python
quality_monitoring_system = {
    'automatic_metrics': {
        'bleu_score': 'è‡ªåŠ¨è¯„ä¼°ç¿»è¯‘è´¨é‡',
        'meteor_score': 'è¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°',
        'ter_score': 'ç¼–è¾‘è·ç¦»è¯„ä¼°'
    },
    'human_evaluation': {
        'fluency': 'æµç•…åº¦äººå·¥è¯„ä¼°',
        'adequacy': 'å‡†ç¡®æ€§äººå·¥è¯„ä¼°',
        'preference': 'ç”¨æˆ·åå¥½æµ‹è¯•'
    },
    'online_monitoring': {
        'latency_tracking': 'å®æ—¶å»¶è¿Ÿç›‘æ§',
        'throughput_monitoring': 'ååé‡ç›‘æ§',
        'error_rate_tracking': 'é”™è¯¯ç‡è·Ÿè¸ª'
    }
}
```

### 8.4 æœªæ¥å‘å±•æ–¹å‘

#### 8.4.1 æŠ€æœ¯å‘å±•è¶‹åŠ¿

1. **ç¥ç»æ¶æ„æœç´¢(NAS)é›†æˆ**:
   - è‡ªåŠ¨æœç´¢æœ€ä¼˜å­¦ç”Ÿæ¨¡å‹æ¶æ„
   - ç¡¬ä»¶æ„ŸçŸ¥çš„æ¶æ„ä¼˜åŒ–
   - å¤šç›®æ ‡ä¼˜åŒ–å¹³è¡¡è´¨é‡å’Œæ•ˆç‡

2. **è”é‚¦å­¦ä¹ ä¸è’¸é¦ç»“åˆ**:
   - åˆ†å¸ƒå¼çŸ¥è¯†è’¸é¦è®­ç»ƒ
   - éšç§ä¿æŠ¤çš„æ¨¡å‹å‹ç¼©
   - è·¨è®¾å¤‡ååŒä¼˜åŒ–

3. **æŒç»­å­¦ä¹ ä¸è’¸é¦**:
   - åœ¨çº¿æ¨¡å‹æ›´æ–°å’Œå‹ç¼©
   - å¢é‡çŸ¥è¯†è’¸é¦
   - ç¾éš¾æ€§é—å¿˜ç¼“è§£

#### 8.4.2 åº”ç”¨é¢†åŸŸæ‰©å±•

1. **å¤šæ¨¡æ€ç¿»è¯‘**:
   - å›¾åƒ-æ–‡æœ¬ç¿»è¯‘è’¸é¦
   - è¯­éŸ³-æ–‡æœ¬ç¿»è¯‘å‹ç¼©
   - è§†é¢‘å­—å¹•ç”Ÿæˆä¼˜åŒ–

2. **é¢†åŸŸè‡ªé€‚åº”è’¸é¦**:
   - é€šç”¨æ¨¡å‹åˆ°ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†è¿ç§»
   - å°‘æ ·æœ¬å­¦ä¹ ä¸è’¸é¦ç»“åˆ
   - é›¶æ ·æœ¬è·¨è¯­è¨€è’¸é¦

3. **å®æ—¶äº¤äº’åº”ç”¨**:
   - åŒå£°ä¼ è¯‘ç³»ç»Ÿä¼˜åŒ–
   - å¯¹è¯ç³»ç»Ÿå“åº”åŠ é€Ÿ
   - å®æ—¶å­—å¹•ç”Ÿæˆ

#### 8.4.3 ç†è®ºç ”ç©¶æ–¹å‘

1. **è’¸é¦æœºåˆ¶ç†è§£**:
   - çŸ¥è¯†ä¼ é€’çš„ä¿¡æ¯è®ºåˆ†æ
   - æš—çŸ¥è¯†çš„æ•°å­¦å»ºæ¨¡
   - è’¸é¦è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ç ”ç©¶

2. **ä¼˜åŒ–ç®—æ³•æ”¹è¿›**:
   - æ›´é«˜æ•ˆçš„è’¸é¦æŸå¤±å‡½æ•°
   - è‡ªé€‚åº”æƒé‡è°ƒæ•´ç®—æ³•
   - å¤šé˜¶æ®µè’¸é¦ä¼˜åŒ–ç­–ç•¥

3. **è¯„ä¼°ä½“ç³»å®Œå–„**:
   - æ›´å…¨é¢çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡
   - æ•ˆç‡-è´¨é‡æƒè¡¡çš„é‡åŒ–æ–¹æ³•
   - é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›è¯„ä¼°

### 8.5 ç»“è®º

çŸ¥è¯†è’¸é¦æŠ€æœ¯ä¸ºTransformeræœºå™¨ç¿»è¯‘æ¨¡å‹çš„å®ç”¨åŒ–éƒ¨ç½²æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„ç†è®ºåˆ†æã€ç²¾å¿ƒè®¾è®¡çš„å®ç°æ–¹æ¡ˆå’Œå…¨é¢çš„å®éªŒéªŒè¯ï¼Œæœ¬æ–‡å±•ç¤ºäº†çŸ¥è¯†è’¸é¦åœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚

**ä¸»è¦è´¡çŒ®**:
1. æä¾›äº†å®Œæ•´çš„TransformerçŸ¥è¯†è’¸é¦ç†è®ºæ¡†æ¶
2. å®ç°äº†å¤šå±‚æ¬¡ã€å¤šç­–ç•¥çš„è’¸é¦æŠ€æœ¯æ–¹æ¡ˆ
3. å»ºç«‹äº†ç³»ç»Ÿæ€§çš„è¯„ä¼°å’Œä¼˜åŒ–ä½“ç³»
4. ä¸ºäº§ä¸šåŒ–åº”ç”¨æä¾›äº†å®ç”¨æŒ‡å¯¼

**æŠ€æœ¯ä»·å€¼**:
- æ˜¾è‘—é™ä½æ¨¡å‹éƒ¨ç½²æˆæœ¬
- å¤§å¹…æå‡æ¨ç†æ•ˆç‡
- ä¿æŒé«˜è´¨é‡çš„ç¿»è¯‘æ€§èƒ½
- æ¨åŠ¨æœºå™¨ç¿»è¯‘æŠ€æœ¯æ™®åŠ

éšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒçŸ¥è¯†è’¸é¦å°†åœ¨æ¨¡å‹å‹ç¼©ã€è¾¹ç¼˜è®¡ç®—ã€ç§»åŠ¨åº”ç”¨ç­‰é¢†åŸŸå‘æŒ¥è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ï¼Œä¸ºæ„å»ºæ›´åŠ é«˜æ•ˆã€å®ç”¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚

---

**å‚è€ƒæ–‡çŒ®**:
1. Hinton, G., et al. "Distilling the Knowledge in a Neural Network." NIPS 2014.
2. Vaswani, A., et al. "Attention is All You Need." NIPS 2017.
3. Sanh, V., et al. "DistilBERT, a distilled version of BERT." arXiv 2019.
4. Jiao, X., et al. "TinyBERT: Distilling BERT for Natural Language Understanding." EMNLP 2020.
5. Wang, W., et al. "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression." NeurIPS 2020.

**ä»£ç ä»“åº“**: [GitHubé“¾æ¥]
**åœ¨çº¿æ¼”ç¤º**: [Demoé“¾æ¥]
**æŠ€æœ¯åšå®¢**: [Blogé“¾æ¥]
